---
emoji: 🔮
title: 7. Gradient Descent 고급편
date: '2023-02-02 10:00:00'
author: tanny
tags: 
categories: AI/ML/DL
---

## 0. 지난 이야기

[이전글](https://tannybrown.github.io/ai/7/)에서는 SGD, mini-batch SGD에 대해서 알아보았다.<br>
이번글에서는 mini-batch에서 발전된 알고리즘들을 알아보자.
<br><br>


## 1. Momentum SGD
mini-batch SGD의 단점은 뭐가 있을까? batch사이즈를 키웠다고 하더라도, GD(Gradient Descent)보다 optimal한 값에 '정확한' 방향으로 나아가진 않는다는 점일 것이다.<br>
그림을 통해 확인해보면 다음과 같다.
<br>
![image](https://user-images.githubusercontent.com/121401159/216329482-e6ac4581-3e14-4c80-ac14-a002fec74982.png)<br>
좌우로 너무 많이 흔들리지 않는가? 이러한 단점을 보완하고자 나온것이 Momentum SGD이다. <br>
momentum SGD을 직관적으로 설명하자면, 관성을 고려하는 update 방법이라고 말할 수 있다. <br>
즉 이전의 gradient들을 고려해서 가중치를 업데이트하는 방법이다.<br>
![image](https://user-images.githubusercontent.com/121401159/216331203-86f04a3e-1ebc-4b8f-82c0-71aedc3a5a4a.png)<br>
그리고 여기서, optimal 한 값에 거의 도달했을때는 관성(?)때문에 빙글빙글 돌게 된다.<br>
따라서, momentum SGD의 경우 과적합(overfitting)을 방지하고 학습을 개선하는 효과가 있다.


## 2. RMSProp
이번엔 또 다른 알고리즘을 살펴보자. <br>
RMSProp(Root Mean Square Propagation)은 지수이동평균(EMA)을 사용하여 각 파라미터의 경사 제곱 값을 계산하여, 경사의 지수이동평균을 적용하여 가중치를 업데이트한다. RMSProp은 **학습률**도 조절하는 기법인데, Gradient의 변화가 큰 파라미터에 대해서는 학습률을 작게 조절하고, Gradient의 변화가 작은 파라미터에 대해서는 학습률을 크게 조절한다. 이를 통해 각 파라미터에 대한 **학습의 안정성**을 높이고, **학습 속도를 개선**할 수 있다.<br>
<br>
잘모르겠다고? 그럴 것 같아서 한번더 정리하면 다음과 같다.<br>
1. Gradient 계산 : 현재 상태의 파라미터에 대한 Gradient를 계산.
2. 지수이동평균 계산 : 각 파라미터에 대한 gradient 제곱 값의 EMA를 계산한다.
3. 학습률 계산 : EMA를 이용해서 각 파라미터에 대한 학습률을 계산한다.
4. 파라미터 업데이트 : 계산된 학습률을 이용하여 파라미터를 업데이트한다.

> 초심자의 경우, 이 부분은 넘겨도 좋다. 왜 쓰는지 정도만 이해하도록 하자.

```
예를 들어보자.
현재 파라미터의 값이 [0.8, 1.2, 0.5]이고, 경사 값이 [0.1, -0.3, 0.2]일 때, EMA는 다음과 같이 구할 수 있다.
첫 번째 파라미터(0.8)의 경사 제곱 값: 0.1^2 = 0.01 
두 번째 파라미터(1.2)의 경사 제곱 값: -0.3^2 = 0.09 
세 번째 파라미터(0.5)의 경사 제곱 값: 0.2^2 = 0.04 

첫 번째 파라미터의 EMA(0.01)은 0.01 * 0.9 + 0.01 * 0.1 = 0.009
두 번째 파라미터의 EMA(0.09)은 0.09 * 0.9 + 0.09 * 0.1 = 0.081
세 번째 파라미터의 EMA(0.04)은 0.04 * 0.9 + 0.02 * 0.1 = 0.038

학습률은 다음과 같이 계산이 가능하다.
첫 번째 파라미터의 학습률: 0.01 / (0.009 + ε)^0.5 = 0.1
두 번째 파라미터의 학습률: 0.09 / (0.081 + ε)^0.5 = 0.3
세 번째 파라미터의 학습률: 0.02 / (0.038 + ε)^0.5 = 0.2
ε는 아주 작은 값으로 수치 안정성을 보장하는 목적으로 사용된다.

그리고, 이제 학습률을 이용하여 파라미터를 업데이트할 수 있다.
첫 번째 파라미터: 0.8 - 0.1 * 0.1 = 0.79
두 번째 파라미터: 1.2 - 0.3 * 0.3 = 1.11
세 번째 파라미터: 0.5 - 0.2 * 0.2 = 0.48
```

<br>
<br>

## 3. Adam
Adam은 앞서 살펴본 Momentum과 RMSProp의 장점을 합친 알고리즘이다. 따라서 가장 많이 사용되는 알고리즘이라고 볼 수 있다.<br>
즉, moment도 계산하고, 학습률도 변화시켜주는 알고리즘이다.<br>
현 단계에서는 자세한 수식은 접어두고 이정도의 개념만 알아두도록 하자.<br>
<br><br>



## 4. 마무리
![image](https://user-images.githubusercontent.com/121401159/216343301-4c2a4fcf-3f78-4612-b779-781576e7d801.png)<br>

이번시간엔 mini-batch에서 발전된 3가지 알고리즘에 대해 살펴보았다.<br>
다음글에서는 Training data와 Test data에 대해 알아보자.

```toc

```