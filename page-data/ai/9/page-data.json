{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/9/",
    "result": {"data":{"cur":{"id":"c64cb4d0-2f25-5b30-837f-860f17272f36","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/216770908-5015748a-9c27-49df-8bd3-760985b446b0.png\" alt=\"image\"><br></p>\n<p><a href=\"https://tannybrown.github.io/ai/8/\">이전글</a>에서 우리는 mini-batch GD에서 업그레이드 된 RMSProp, Momentum, Adam에 대해 알아보았다.<br>\r\n이번글에서는 머신러닝, 딥러닝에서 사용되는 데이터들을 어떻게 부르는지에 대해 알아보고자 한다.<br></p>\n<p><br><br></p>\n<h2 id=\"1-training-vs-test-vs-validation\" style=\"position:relative;\"><a href=\"#1-training-vs-test-vs-validation\" aria-label=\"1 training vs test vs validation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Training vs Test vs Validation</h2>\n<p>본론으로 들어가기전에, 하나의 예시를 들어보겠다.<br>\r\n아마 이글을 읽는 대대분의 사람은 대입을 준비하는 수험생이었던 적이 있을 것이다.<br>\r\n대입 수험생의 공부는 크게 3가지 파트로 나눌 수 있다.<br></p>\n<ol>\n<li>우리는 수험생 때, 서점에서 문제집을 사서 <strong>문제집</strong>을 푼다.(ex. EBS 수능특강,수능완성 등등)<br></li>\n<li>그리고 6월 9월에 전국 <strong>모의고사</strong>를 본다.<br></li>\n<li>마지막으로 11월 <strong>수능</strong>을 치른다. <br></li>\n</ol>\n<p>볼드체로 표시된 단어를 머신러닝, 딥러닝에 쓰이는 데이터에 mapping시킨다면 다음과 같다.<br></p>\n<ol>\n<li>문제집 -> Training data</li>\n<li>모의고사 -> Validation data</li>\n<li>수능 -> Test data</li>\n</ol>\n<br>\r\n가장 먼저 training data는 모델 학습에 쓰인는 데이터로, 이 데이터를 통해 모델은 학습을 한다.<br>\r\n모델의 학습이 끝났다면, 우리는 test data를 통해서 새로운 데이터가 주어졌을 때 적절한 output을 내는지 확인할 수 있다.<br>\r\n그렇다면 training, test data만 있으면 될 것 같은데 validation data는 왜 필요한 것일까?<br>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/216772311-bbb37f2b-16d6-4975-b5e1-d1bf09cb0cc7.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>여전히 훌륭한 퀄리티의 그림이다.</p>\n</blockquote>\n<p>자 위 그래프를 보면 x축이 총 epoch수, y축이 loss값이다. 우리가 training data로 여러번(많은 epoch) 학습을 수행하면, 점점 loss가 줄어서 거의 오차가 없는 것을 확인할 수 있다.<br>\r\n그렇다면 이렇게 학습을 시키면 test 데이터에 대해서 적절한 결과가 도출될까?<br>\r\n아니다. 이 경우 과적합(overfitting)으로 이어지게 되며, test 데이터에 대해서 성능이 매우 좋지 않게 된다.<br>\r\n따라서 우리는 training 데이터와 새로운 데이터 간의 적절한 절충안을 알아야한다.<br>\r\n이러한 절충안을 알기 위해 나온 것이 바로, validation data이다.<br>\r\ntest data를 통한 test전에, validation data를 통해서 모델의 성능을 평가하여 개발자들은 모델이 실제 새로운 데이터에서 어느 정도의 성능을 보일 것인지에 대한 통찰력을 얻을 수 있다.<br>\r\n오버피팅이 일어나지 않는 적절한 epoch, 성능이 좋은 모델 아키텍쳐 등 <strong>하이퍼 파라미터</strong>를 짐작해보면서 더 좋은 성능의 모델을 만들 수 있는 것이다.<br>\r\n<br><br></p>\n<h2 id=\"2-k-fold-cross-validation\" style=\"position:relative;\"><a href=\"#2-k-fold-cross-validation\" aria-label=\"2 k fold cross validation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. K-fold Cross validation</h2>\n<p>training, test, validation 까지 알아보았다. 그런데 여기서 문제가 하나 발생한다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216772877-2804729f-ee4e-477a-ac25-1e3c439373d8.png\" alt=\"image\"><br>\r\n바로 데이터가 부족하다는 것이다.<br>\r\n데이터만 충분하다면 training, test, validation 데이터를 적절히 나눠서 학습을 진행하면 될 것이다. 허나, 대부분의 경우 우리에게 주어진 데이터는 충분치 못하다.<br>\r\n자 상황을 가정해보자. 우리에게 training data가 100개가 있고 test data가 20개 있다.<br>\r\ntest data는 수능문제니까 건들면 안된다. 그럼 validation data확보를 위해선 training data에 손을 대야한다. 허나 training data도 넉넉치 않은 상황이다.<br>\r\n이때 사용하는 방법 중 하나가 K-fold cross validation이다.<br></p>\n<blockquote>\n<p>k - fold cross validation은 데이터 집합을 k개의 독립적인 폴드로 나누어 k번의 훈련과 평가를 수행하는 것이다. 각 폴드는 훈련 데이터 집합에서 검증 데이터 집합으로 사용되고, k-1개의 폴드는 훈련에 사용된다. k번의 훈련과 평가 과정을 모두 수행한 후, 각 폴드에서의 성능을 평균하여 최종적인 모델의 성능을 평가한다.</p>\n</blockquote>\n<p>지금은 5-fold cross validation을 적용해보자.<br>\r\n100개의 데이터가 있으니, 5개의 fold로 나눈다. 각 폴드는 20개의 데이터로 구성될 것이다. <br>\r\n이제 총 5가지 훈련과 평가 수행한다.</p>\n<blockquote>\n<p>이때 중요한 것이, 아래의 학습은 같은 하이퍼파라미터를 가진 모델이어야 한다.</p>\n</blockquote>\n<ol>\n<li>첫번째 폴드를 validation 으로 사용하고, 나머지 4개의 폴드로 훈련을 시킨다.</li>\n<li>두번째 폴드를 validation 으로 사용하고, 나머지 4개의 폴드로 훈련을 시킨다.</li>\n<li>세번째 폴드를 validation 으로 사용하고, 나머지 4개의 폴드로 훈련을 시킨다.</li>\n<li>네번째 폴드를 validation 으로 사용하고, 나머지 4개의 폴드로 훈련을 시킨다.</li>\n<li>다섯번째 폴드를 validation 으로 사용하고, 나머지 4개의 폴드로 훈련을 시킨다.</li>\n</ol>\n<p>1~5번의 결과를 평균내어, 이러한 하이퍼 파라미터일때의 성능은 이러하다. 라는 결론을 내린다.<br></p>\n<blockquote>\n<p>이렇게 하나의 하이퍼파라미터 세트를 갖고 훈련을 진행하는 것이다.<br> 이 방법을 다른 하이퍼 파라미터 셋에도 적용을 해보고, 서로 다른 하이퍼 파라미터 셋들에 대해서 무엇이 더 성능이 좋은지 확인할 수 있는 것이다.</p>\n</blockquote>\n<br>\n<p>추가로, k-fold cross validation은 데이터 집합의 불균형 해결을 해결할 수 있다.<br>\r\n특정 데이터 포인트나 카테고리가 너무 많이 포함된 경우, 모델의 성능이 떨어질 수 있다. 예를 들어, 강아지 고양이 사진을 분류하는 모델을 만들때, 우리가 설정한 validation data에 강아지 사진만으로 가득하다면, 정확한 validation이 불가할 것이다. 이러한 문제를 k-fold cross validation을 통해서 해결할 수 있는 것이다.<br></p>\n<blockquote>\n<p>또한, overfitting을 방지하는데도 효과가 있다. 특정 데이터가 아닌, 모든 데이터에 대해 학습을 진행하기 때문이다.</p>\n</blockquote>\n<p><br><br></p>\n<h2 id=\"3-마무리\" style=\"position:relative;\"><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"3 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/216774441-f6cb1fe0-c28d-42b4-8ae4-3b139e84bd0b.png\" alt=\"image\"><br></p>\n<p>이번글에서는 학습에 쓰이는 데이터들과 k-fold cross validation에 대해 알아보았다.<br>\r\n다음글에서는 역전파에 대해 알아보자.<br></p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-training-vs-test-vs-validation\">1. Training vs Test vs Validation</a></li>\n<li><a href=\"#2-k-fold-cross-validation\">2. K-fold Cross validation</a></li>\n<li><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\">3. 마무리</a></li>\n</ul>\n</div>","excerpt":"0. 지난 이야기 image 이전글에서 우리는 mini-batch GD에서 업그레이드 된 RMSProp, Momentum, Adam에 대해 알아보았다.\r\n이번글에서는 머신러닝, 딥러닝에서 사용되는 데이터들을 어떻게 부르는지에 대해 알아보고자 한다.  1. Training vs Test vs Validation 본론으로 들어가기전에, 하나의 예시를 들어보겠다.\r\n아마 이글을 읽는 대대분의 사람은 대입을 준비하는 수험생이었던 적이 있을 것이다.\r\n대입 수험생의 공부는 크게 3가지 파트로 나눌 수 있다. 우리는 수험생 때, 서점에서 문제집을 사서 문제집을 푼다.(ex. EBS 수능특강,수능완성 등등) 그리고 6월 9월에 전국 모의고사를 본다. 마지막으로 11월 수능을 치른다.  볼드체로 표시된 단어를 머신러닝, 딥러닝에 쓰이는 데이터에 mapping시킨다면 다음과 같다. 문제집 -> Training data 모의고사 -> Validation data 수능 -> Test data imag…","frontmatter":{"date":"February 04, 2023","title":"8. Training Data & Test Data","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/9/"}},"next":{"id":"25788e54-44ec-55cb-944e-4a91990dddc8","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/7/\">이전글</a>에서는 SGD, mini-batch SGD에 대해서 알아보았다.<br>\r\n이번글에서는 mini-batch에서 발전된 알고리즘들을 알아보자.\r\n<br><br></p>\n<h2 id=\"1-momentum-sgd\" style=\"position:relative;\"><a href=\"#1-momentum-sgd\" aria-label=\"1 momentum sgd permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Momentum SGD</h2>\n<p>mini-batch SGD의 단점은 뭐가 있을까? batch사이즈를 키웠다고 하더라도, GD(Gradient Descent)보다 optimal한 값에 ‘정확한’ 방향으로 나아가진 않는다는 점일 것이다.<br>\r\n그림을 통해 확인해보면 다음과 같다.\r\n<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216329482-e6ac4581-3e14-4c80-ac14-a002fec74982.png\" alt=\"image\"><br>\r\n좌우로 너무 많이 흔들리지 않는가? 이러한 단점을 보완하고자 나온것이 Momentum SGD이다. <br>\r\nmomentum SGD을 직관적으로 설명하자면, 관성을 고려하는 update 방법이라고 말할 수 있다. <br>\r\n즉 이전의 gradient들을 고려해서 가중치를 업데이트하는 방법이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216331203-86f04a3e-1ebc-4b8f-82c0-71aedc3a5a4a.png\" alt=\"image\"><br>\r\n그리고 여기서, optimal 한 값에 거의 도달했을때는 관성(?)때문에 빙글빙글 돌게 된다.<br>\r\n따라서, momentum SGD의 경우 과적합(overfitting)을 방지하고 학습을 개선하는 효과가 있다.</p>\n<h2 id=\"2-rmsprop\" style=\"position:relative;\"><a href=\"#2-rmsprop\" aria-label=\"2 rmsprop permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. RMSProp</h2>\n<p>이번엔 또 다른 알고리즘을 살펴보자. <br>\r\nRMSProp(Root Mean Square Propagation)은 지수이동평균(EMA)을 사용하여 각 파라미터의 경사 제곱 값을 계산하여, 경사의 지수이동평균을 적용하여 가중치를 업데이트한다. RMSProp은 <strong>학습률</strong>도 조절하는 기법인데, Gradient의 변화가 큰 파라미터에 대해서는 학습률을 작게 조절하고, Gradient의 변화가 작은 파라미터에 대해서는 학습률을 크게 조절한다. 이를 통해 각 파라미터에 대한 <strong>학습의 안정성</strong>을 높이고, <strong>학습 속도를 개선</strong>할 수 있다.<br>\r\n<br>\r\n잘모르겠다고? 그럴 것 같아서 한번더 정리하면 다음과 같다.<br></p>\n<ol>\n<li>Gradient 계산 : 현재 상태의 파라미터에 대한 Gradient를 계산.</li>\n<li>지수이동평균 계산 : 각 파라미터에 대한 gradient 제곱 값의 EMA를 계산한다.</li>\n<li>학습률 계산 : EMA를 이용해서 각 파라미터에 대한 학습률을 계산한다.</li>\n<li>파라미터 업데이트 : 계산된 학습률을 이용하여 파라미터를 업데이트한다.</li>\n</ol>\n<blockquote>\n<p>초심자의 경우, 이 부분은 넘겨도 좋다. 왜 쓰는지 정도만 이해하도록 하자.</p>\n</blockquote>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">예를 들어보자.\r\n현재 파라미터의 값이 [0.8, 1.2, 0.5]이고, 경사 값이 [0.1, -0.3, 0.2]일 때, EMA는 다음과 같이 구할 수 있다.\r\n첫 번째 파라미터(0.8)의 경사 제곱 값: 0.1^2 = 0.01 \r\n두 번째 파라미터(1.2)의 경사 제곱 값: -0.3^2 = 0.09 \r\n세 번째 파라미터(0.5)의 경사 제곱 값: 0.2^2 = 0.04 \r\n\r\n첫 번째 파라미터의 EMA(0.01)은 0.01 * 0.9 + 0.01 * 0.1 = 0.009\r\n두 번째 파라미터의 EMA(0.09)은 0.09 * 0.9 + 0.09 * 0.1 = 0.081\r\n세 번째 파라미터의 EMA(0.04)은 0.04 * 0.9 + 0.02 * 0.1 = 0.038\r\n\r\n학습률은 다음과 같이 계산이 가능하다.\r\n첫 번째 파라미터의 학습률: 0.01 / (0.009 + ε)^0.5 = 0.1\r\n두 번째 파라미터의 학습률: 0.09 / (0.081 + ε)^0.5 = 0.3\r\n세 번째 파라미터의 학습률: 0.02 / (0.038 + ε)^0.5 = 0.2\r\nε는 아주 작은 값으로 수치 안정성을 보장하는 목적으로 사용된다.\r\n\r\n그리고, 이제 학습률을 이용하여 파라미터를 업데이트할 수 있다.\r\n첫 번째 파라미터: 0.8 - 0.1 * 0.1 = 0.79\r\n두 번째 파라미터: 1.2 - 0.3 * 0.3 = 1.11\r\n세 번째 파라미터: 0.5 - 0.2 * 0.2 = 0.48</code></pre></div>\n<br>\r\n<br>\n<h2 id=\"3-adam\" style=\"position:relative;\"><a href=\"#3-adam\" aria-label=\"3 adam permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Adam</h2>\n<p>Adam은 앞서 살펴본 Momentum과 RMSProp의 장점을 합친 알고리즘이다. 따라서 가장 많이 사용되는 알고리즘이라고 볼 수 있다.<br>\r\n즉, moment도 계산하고, 학습률도 변화시켜주는 알고리즘이다.<br>\r\n현 단계에서는 자세한 수식은 접어두고 이정도의 개념만 알아두도록 하자.<br>\r\n<br><br></p>\n<h2 id=\"4-마무리\" style=\"position:relative;\"><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"4 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/216343301-4c2a4fcf-3f78-4612-b779-781576e7d801.png\" alt=\"image\"><br></p>\n<p>이번시간엔 mini-batch에서 발전된 3가지 알고리즘에 대해 살펴보았다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/9/\">다음글</a>에서는 Training data와 Test data에 대해 알아보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-momentum-sgd\">1. Momentum SGD</a></li>\n<li><a href=\"#2-rmsprop\">2. RMSProp</a></li>\n<li><a href=\"#3-adam\">3. Adam</a></li>\n<li><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\">4. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"February 02, 2023","title":"7. Gradient Descent 고급편","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/8/"}},"prev":{"id":"ad4361ab-0e71-5f15-bf7b-296828c05c55","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/9/\">이전글</a>에서 우린 학습에 이용되는 데이터에 대해 다뤄보았다.<br>\r\n이번글에서는 딥러닝에서 뺴놓고 이야기할 수 없는 역전파(Back-Propagation)에 대해 이야기 해보고자 한다.<br></p>\n<p><br><br></p>\n<h2 id=\"1-순전파\" style=\"position:relative;\"><a href=\"#1-%EC%88%9C%EC%A0%84%ED%8C%8C\" aria-label=\"1 순전파 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 순전파</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/216831403-ebfd4bd0-5757-487e-9426-f95bd85a83e2.png\" alt=\"image\"> <br></p>\n<p>역전파를 설명하기 전에, 순전파에 대해 먼저 짚고 가야한다. <br>\r\n갑자기 새로운 용어가 나오니 당황스럽겠지만 지금까지 배운 내용에 대한 정리와 다름이 없으니 무리없이 따라올 수 있을 것이다.<br>\r\nDNN을 기억하는가? Deep Neural Network, 깊은 인공신경망을 의미했다. <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/217760131-8cf8bdc2-bbb4-478b-813a-917784e00720.png\" alt=\"image\">\r\n<br>\r\n자 위에 간단한 구조의 인공신경망이 있다. 이때, 입력값으로 x1 = 2, x2 = 1 이 주어지고, f1,f2,f3,f4는 unit step function이라고 해보자.<br>\r\n그렇다면 계산은 다음과 같이 진행될 것이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/217760693-838144b0-195e-4fe2-9305-a252ae410b86.png\" alt=\"image\">\r\n<br>\r\n앞서 배운 바와 같이, 가중치와 입력값을 곱한후 더하여 activation function을 거치는 방식으로 계산하면 Y는 1이 출력된다.<br>\r\n이런식으로 <strong>입력을 통해서 출력값을 구하는 것</strong>을 순전파(Forward-Propagation,Forward-Pass)라고 한다.<br></p>\n<p><br><br></p>\n<h2 id=\"2-역전파\" style=\"position:relative;\"><a href=\"#2-%EC%97%AD%EC%A0%84%ED%8C%8C\" aria-label=\"2 역전파 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 역전파</h2>\n<p>하지만 학습이라는 것은 여기서 끝나는 것이 아니었다.<br>\r\n우리는 이렇게 구한 출력값을 통해서 Loss를 구하고,Loss를 최소로해주는 파라미터로 업데이트를 해줘야한다.<br>\r\n파라미터를 업데이트하는 과정이 바로 back-propagation 인데, 이는 chain rule을 이용해서 계산이 된다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/217763519-5c3f81a0-7b59-4368-af62-d3aa2a76577a.png\" alt=\"image\"><br>\r\n아까와 같은 예시에서 확인해보자. 각 노드에 들어가는 입력값을 p라고 하고, 노드에서 나온출력을 q라고 해보겠다.<br>\r\n파라미터 업데이트하는 식을 떠올려보면, Loss function을 w(가중치)로 미분한 Gradient를 구해야했다.<br>\r\n이때 chain rule이 쓰이는데 다음과 같은 형태로 구할 수 있다.<br><br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/217765247-9e7aba0c-2735-4971-a4d3-eca0a0b10ee2.png\" alt=\"image\"><br>\r\n이렇게 가장 마지막 층의 가중치를 chain rule을 통해서 update할 수 있다. 이렇게 마지막 층부터 다시 첫번째 층까지 <strong>역순으로 가중치를 업데이트</strong> 한다고 하여 역전파인 것이다.</p>\n<blockquote>\n<p>이렇게만 쓰면 어떻게 알아요!</p>\n</blockquote>\n<p>이제 단순히 미분을 해주면 되는데, p2 = wq1+~라는 관계,q2 = f(p2)라는 관계를 이용하면 쉽게 미분해서 구할 수 있을 것이다.<br>\r\n이제 계산은 당신의 몫이다.<br></p>\n<br>\n<h3 id=\"보충-조금-더-깊은-layer의-weight\" style=\"position:relative;\"><a href=\"#%EB%B3%B4%EC%B6%A9-%EC%A1%B0%EA%B8%88-%EB%8D%94-%EA%B9%8A%EC%9D%80-layer%EC%9D%98-weight\" aria-label=\"보충 조금 더 깊은 layer의 weight permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(보충) 조금 더 깊은 Layer의 weight..</h3>\n<p>그런데 여기서 어렴풋이 알고 넘어가면 틀리는 함정이 하나 숨겨져 있다.<br>\r\n가장 마지막 가중치는 저렇게 구할 수 있었는데, 더 앞쪽 layer의 가중치는 다소 다르다. <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/217772885-fea3b2a6-4996-4b9f-a7be-2ef30201f578.png\" alt=\"image\"><br>\r\n앞선 예시에서 마지막 layer에 노드를 하나 추가했다. 이때, w11을 업데이트 하고 싶다면 어떻게 구할 수 있을까?<br>\r\n아까와 같은 방식으로, chain rule을 이용한다면,<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/217772381-ec4f932c-ad68-4212-99ce-1e3bec5a8d8f.png\" alt=\"image\">\r\n<br></p>\n<p>라고 쓸 수 있다. 혹시 이렇게 생각한 분이 있다면 정말 다행이다. 왜냐하면 여기서 추가해줘야하는 식이 더 있기 때문이다.<br>\r\n자 생각해보자. w11이라는 가중치는 p1에 영향을 준다. p1은 q1에 영향을 준다. q1은 p2에, p2는 q2에, q2는 L에 영향을 준다.<br>\r\n이렇게만 영향을 준다면 위 식은 맞다. 하지만 빠진게 있다. q1은 p2뿐아니라 p3에도 영향을 준다. 그리고 p3는 q3에 영향을 주고, q3는 L에 영향을 준다.<br>\r\n즉 w11이라는 가중치를 업데이트하기위해서는 다음과 같은 식으로 Gradient를 구해야할 것이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/217772743-9611e824-a7e7-4cbb-88d9-c0b03e43abc4.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>왜 더하기 입니까?</p>\n</blockquote>\n<p>혹자는 왜 두식을 더하는 것인지 물어볼 수 있다. 이에 대한 자세한 설명은 생략하지만, 미분의 정의를 토대로 생각해보면 덧셈으로 정리되는 식이 도출되니 한번 증명해보는 것도 좋을 것 같다.</p>\n<p><br><br></p>\n<h2 id=\"3-마무리\" style=\"position:relative;\"><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"3 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/217774241-fa17e450-2a13-49b7-b0c6-cf8095743426.png\" alt=\"image\"><br></p>\n<p>이번시간에는 순전파, 역전파에 대해 살펴보았다.<br>\r\n다음글에서는 이진분류로 돌아오겠다.<br></p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></p>\n</li>\n<li>\n<p><a href=\"#1-%EC%88%9C%EC%A0%84%ED%8C%8C\">1. 순전파</a></p>\n</li>\n<li>\n<p><a href=\"#2-%EC%97%AD%EC%A0%84%ED%8C%8C\">2. 역전파</a></p>\n<ul>\n<li><a href=\"#%EB%B3%B4%EC%B6%A9-%EC%A1%B0%EA%B8%88-%EB%8D%94-%EA%B9%8A%EC%9D%80-layer%EC%9D%98-weight\">(보충) 조금 더 깊은 Layer의 weight..</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\">3. 마무리</a></p>\n</li>\n</ul>\n</div>","frontmatter":{"date":"February 05, 2023","title":"9. 딥러닝 업데이트 흐름 알기","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/10/"}},"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/9/","nextSlug":"/ai/8/","prevSlug":"/ai/10/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}