{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/6/",
    "result": {"data":{"cur":{"id":"e9cd9f14-c115-5b0b-a667-0d8058e4739a","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/216059096-c5eeb283-5e96-42f6-bd15-7c64ec2b3137.png\" alt=\"image\">\r\n<br>\r\n<a href=\"https://tannybrown.github.io/ai/5/\">이전글</a>에서 선형회귀와 Loss등에 대해 알아보았다. <br>\r\n그리고 우린, 스마트한 방법으로 optimal한 파라미터와 bias를 찾기 위해 Numerical Method를 사용한다는 것까지 알아봤다. <br>\r\n그리고 오늘은 Numerical Method인 Gradient Descent에 대해 알아보자.</p>\n<p><br><br></p>\n<h2 id=\"1-직관적-이해\" style=\"position:relative;\"><a href=\"#1-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4\" aria-label=\"1 직관적 이해 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 직관적 이해</h2>\n<p>Gradient Descent는 직역하면, 경사하강법이다.<br> 이 방법은 <strong>Loss가 줄어드는 방향</strong>을 찾아 파라미터와 bias를 업데이트 하는 방법이라고 할 수 있다.<br>\r\n예시를 통한 직관적 이해를 해보자.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216073365-d46b5057-690c-4186-b7b8-9727fefb9778.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>그림퀄리티가 상당하다 <br></p>\n</blockquote>\n<p>자, 만약 위 그림에서 최저점을 찾고싶다고 하자. 그리고 현재 우리의 위치가 <strong>파란색 점</strong>으로 주어졌다고 한다면, 우린 어떤 방향으로 나아가야할까?<br>\r\n여러가지 방법론이 있을 수 있지만, <strong>Gradient Descent</strong>에서는 <strong>현재위치에서 기울기의 반대방향</strong> 으로 나아간다.<br>\r\n즉, 기울기가 양수(+)이므로 (-)방향으로 나아가야한다.<br>\r\n자 그럼 어디갈지 알았으니 한발짝 나가아보자. <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216073715-42b5ea14-97f4-426c-af86-c8782d1b95e6.png\" alt=\"image\">\r\n<br>\r\n엇 이번엔 기울기의 부호가 달라졌다. 현재 기울기의 부호는 (-)이므로 (+)방향으로 가야한다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216074025-07c7dabd-f67f-4f6c-a464-2e824428d45f.png\" alt=\"image\"><br>\r\n드디어 도착했다. 이렇게 기울기가 0이 되면 움직임을 멈추고 최저점을 찾아냈다는 것을 알 수 있다.<br></p>\n<p><br><br>\r\n다소 야매로 그린 예시였지만, 위 그래프가 Loss에 대한 그래프라고 생각한다면, loss가 가장 작아지는 값을 찾고 싶은 우리가 원하는 값에 도달할 수 있었던 것이다.<br>\r\n위 매커니즘을 식으로 표현하면 다음과 같다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216074281-39220e6e-6ced-42ef-bbdf-d43c76437bc3.png\" alt=\"image\"><br>\r\nw는 가중치(파라미터)를 나타내고, L은 Loss function을 의미한다. 입실론은 학습률(learning rate)를 나타내는데, 이는 step-size라고 하기도 한다. 이 step-size에 따라 w를 얼마나 이동시킬지가 달라지며, step-size를 <strong>상수(고정값으로)</strong> 으로 둘 수도 있고 <strong>스케줄링</strong>해서 변하는 값으로 둘 수도 있다.<br></p>\n<blockquote>\n<p>bias는요?</p>\n</blockquote>\n<p>bias도 결국 parameter이다. 즉, 같은 방식인데 bias 에 대해서 같은 과정을 수행하면 된다. 다만 w따로, bias 따로 계산해주는게 귀찮을 수 있다. 따라서 homogeneous equation으로 바꿔서 계산한다면(w의 차원을 하나 올려서) 계산의 편의를 가져갈 수 있다.<br><br></p>\n<h2 id=\"2-한계\" style=\"position:relative;\"><a href=\"#2-%ED%95%9C%EA%B3%84\" aria-label=\"2 한계 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 한계</h2>\n<p>위 글을 따라 오면서 눈치챈 사람이 있을수도 있겠다. Gradient Descent는 명확한 한계를 가진다. <br></p>\n<ul>\n<li>너무 신중하다.</li>\n<li>local minimum <br></li>\n</ul>\n<p>우선 너무 신중하다. 즉 한번의 update를 위해서 갖고 있는 모든 데이터를 이용해 Loss를 계산해야한다. 데이터수가 많아지면 매우매우 느려진다.<br>\r\n두번째로 Local Minimum을 찾는 알고리즘이라는 것이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216072186-48307436-a632-4a7a-9a5d-7eeaa6b781a5.png\" alt=\"image\"><br></p>\n<p>우리가 알고싶은 것은 global minimum이다. 만약 위와 같은 loss가 주어진다면, 원하는 최저값에 도달하지 못할 수 있다.<br>\r\n즉, convex한 문제에서만 적용이 가능하다.</p>\n<blockquote>\n<p>알고리즘 공부를 한사람이라면, 눈치챘을 것이다. Gradient Descent는 greedy 하다.</p>\n</blockquote>\n<p><br><br></p>\n<h2 id=\"3-마무리\" style=\"position:relative;\"><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"3 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 마무리</h2>\n<p>이번글에서는 gradient descent에 대해 알아보았다.<br>\r\n다음글에서는 gradient descent의 보완된 버전들에 대해 알아보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4\">1. 직관적 이해</a></li>\n<li><a href=\"#2-%ED%95%9C%EA%B3%84\">2. 한계</a></li>\n<li><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\">3. 마무리</a></li>\n</ul>\n</div>","excerpt":"0. 지난 이야기 image\r\n\r\n이전글에서 선형회귀와 Loss등에 대해 알아보았다. \r\n그리고 우린, 스마트한 방법으로 optimal한 파라미터와 bias를 찾기 위해 Numerical Method를 사용한다는 것까지 알아봤다. \r\n그리고 오늘은 Numerical Method인 Gradient Descent에 대해 알아보자.  1. 직관적 이해 Gradient Descent는 직역하면, 경사하강법이다. 이 방법은 Loss가 줄어드는 방향을 찾아 파라미터와 bias를 업데이트 하는 방법이라고 할 수 있다.\r\n예시를 통한 직관적 이해를 해보자.\r\nimage 그림퀄리티가 상당하다  자, 만약 위 그림에서 최저점을 찾고싶다고 하자. 그리고 현재 우리의 위치가 파란색 점으로 주어졌다고 한다면, 우린 어떤 방향으로 나아가야할까?\r\n여러가지 방법론이 있을 수 있지만, Gradient Descent에서는 현재위치에서 기울기의 반대방향 으로 나아간다.\r\n즉, 기울기가 양수(+)이므로 (-)방향으…","frontmatter":{"date":"January 29, 2023","title":"5. Gradient Descent","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/6/"}},"next":{"id":"4e9354e9-a82a-5b0d-be7c-aa239d3ec41c","html":"<h2 id=\"0-회귀가-뭐야\" style=\"position:relative;\"><a href=\"#0-%ED%9A%8C%EA%B7%80%EA%B0%80-%EB%AD%90%EC%95%BC\" aria-label=\"0 회귀가 뭐야 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 회귀가 뭐야</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/215163818-4b204570-e3e1-42fd-a655-dfbcc48a5c64.png\" alt=\"image\"><br></p>\n<p><a href=\"https://tannybrown.github.io/ai/4/\">이전글</a>에서 인공신경망에 대해 알아보았다. <br>\r\n이번 글에서는 <strong>선형회귀</strong>에 대해 알아보자. <br>\r\n회귀는 ’<strong>입력과 출력간의 관계를 파악하는 것</strong>‘을 의미한다.<br></p>\n<blockquote>\n<p>회귀를 <strong>왜</strong> 배우나요?</p>\n</blockquote>\n<p>우린 회귀를 통해, <strong>처음보는 입력에 대해서 적절한 출력</strong>을 얻고 싶다.<br>\r\n그럼 선형 회귀는 뭘까?<br>\r\n어떤 <strong>연속적인 데이터 값</strong>들 간의 관계를 선형 함수로 모델링하는 것을 <strong>선형회귀(linear regression)</strong> 라 한다. <br></p>\n<blockquote>\n<p>연속적인 데이터가 뭔가요?</p>\n</blockquote>\n<p>데이터의 분류를 먼저 알아야하는데, 데이터는 크게 <strong>수치형</strong>과 <strong>범주형</strong>으로 나뉜다. <br>\r\n그리고 그 중 수치형은 <strong>연속형</strong>과 <strong>이산형</strong>으로 나뉜다. <br>\r\n예를 들어 몸무게, 키, 시간 등이 연속형에 속하며, 상품의 개수, 글자 수 등이 이산형에 속한다.</p>\n<blockquote>\n<p>??? : 선형이 뭔가요?</p>\n</blockquote>\n<p>선형이라함은, 쉽게 말해 <strong>일차식 관계</strong>를 의미한다. 선형의 의미에 대해서는 다른글에서 따로 다뤄보자. <br></p>\n<p>선형 회귀는 두 가지 유형으로 나눌 수 있다.</p>\n<ul>\n<li>\n<p>일반적인 선형 회귀(Simple Linear Regression) : 하나의 독립 변수(Independent variable)와 하나의 종속 변수(Dependent variable)로 구성된 데이터를 모델링하는 경우. <br>예를 들어, 공부 시간과 시험 점수 간의 관계를 모델링 할 때 사용한다.</p>\n</li>\n<li>\n<p>다중 선형 회귀(Multiple Linear Regression) : 여러 개의 독립 변수와 하나의 종속 변수로 구성된 데이터를 모델링하는 경우. <br>예를 들어, 공부 시간, 인터넷 사용 시간, 휴식 시간과 시험 점수 간의 관계를 모델링 할 때 사용한다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h2 id=\"1-선형회귀와-학습\" style=\"position:relative;\"><a href=\"#1-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%99%80-%ED%95%99%EC%8A%B5\" aria-label=\"1 선형회귀와 학습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 선형회귀와 학습</h2>\n<p>예시를 하나 가져왔다. <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215274142-03f6ea1d-bcea-44f5-adee-0d32613b9aad.png\" alt=\"image\"><br>\r\n(reference : <a href=\"https://gilberttanner.com/blog/linear-regression-explained)\">https://gilberttanner.com/blog/linear-regression-explained)</a><br><br>\r\nx축이 공부시간, y축이 시험 점수이고 다음과 같은 data(점)들이 존재한다고 할때, x와 y의 선형관계를 빨간 직선으로 표현할 수 있다.<br>\r\n선형관계를 잘 표현한 적절한 빨간 직선을 찾는 것이 바로 <strong>학습</strong>이다.<br>\r\n위 직선은 다음과 같이 표현할 수 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215274972-af0916e9-f458-41df-945c-8c28b6e2c7a5.png\" alt=\"image\"><br>\r\ny위의 ^(hat)이 있는 이유는 <strong>실제 y값</strong>이 아닌 <strong>예측된 y값</strong>이라는 의미이다.<br>\r\n또한 앞선 글에서 배운 파라미터와 bias를 확인할 수 있다.<br>\r\n학습 방법은 매우 간단하다. 먼저 Loss(cost)를 정의한다. 이 경우 Loss로 MSE를 써보자.<br></p>\n<blockquote>\n<p>MSE는 mean square error의 약자로, (예측값과 실제 값의 차)의 제곱의 합을 의미한다.</p>\n</blockquote>\n<p>Loss는 이름에서 알 수 있듯, 작을 수록 좋다. 이 Loss의 값을 <strong>최소</strong>로 만드는 w와 b를 찾으면, 학습이 끝난다.<br>\r\n자 그러면, w와 b를 어떻게 찾을 수 있을까?<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215275914-56118e3d-78ed-4549-98ff-010e90810eef.png\" alt=\"image\"><br>\r\n<br>\r\n<strong>무지성 대입</strong>을 시도할 수도 있지만, 지성인으로서 스마트한 방법을 생각해보자.<br>\r\n우린 크게 두가지 방법을 사용할 수 있다.</p>\n<ul>\n<li><strong>Analytical method</strong></li>\n<li><strong>Numerical method</strong></li>\n</ul>\n<p>먼저 Analytic method는 수학적 접근이다. 직접 손으로 식을 써서 수학을 통해 문제를 해결한다. 반대로, Numerical method는 수치해석적 방식으로, 컴퓨터를 이용해 문제를 해결한다.<br>\r\n그리고 우리가 해결할 대부분의 경우에, Numerical method를 사용하게 될 것이다.</p>\n<blockquote>\n<p><strong>왜</strong> Analytical method는 많이 안쓰이나요?</p>\n</blockquote>\n<p>Analytical methods는 수학적 방법을 사용하여 문제를 해결하므로 문제가 수학적으로 표현될 수 있는 경우에만 사용할 수 있다. 하지만 많은 문제들이 수학적으로 표현하기 어렵거나, 계산 시간이 너무 오래 걸린다. 반면 Numerical method는 수학적으로 표현되지 않아도, 컴퓨터로 근사치를 구해낼 수 있다. <br>\r\n또한, 일부 문제는 실제 일어나는 현상을 모델링하는데, 이러한 모델은 수학적으로 정의가 어렵다. ex. 나비에 스토크스 <br>\r\n따라서 Numerical method가 더 많이 쓰인다.</p>\n<blockquote>\n<p>필자는 언제나 말하지만, 어떤 하나의 방법을 고수하는 것은 옳지 않다고 생각한다. 모든 방법엔 trade-off가 있고, <strong>주어진 상황에 알맞은 방법을 취사선택하는 것이 엔지니어가 가져야할 자세</strong>이다.</p>\n</blockquote>\n<p>Numerical method를 통해 w와 b의 값을 찾을 수 있다는 것까지 알았다.<br><br>\r\n다음글에서, 선형회귀에서 쓰이는 Numerical method, <strong>Gradient Descent</strong>에 대해 알아보자!</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%ED%9A%8C%EA%B7%80%EA%B0%80-%EB%AD%90%EC%95%BC\">0. 회귀가 뭐야</a></li>\n<li><a href=\"#1-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%99%80-%ED%95%99%EC%8A%B5\">1. 선형회귀와 학습</a></li>\n</ul>\n</div>","frontmatter":{"date":"January 28, 2023","title":"4. 선형회귀를 하는 이유","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/5/"}},"prev":null,"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/6/","nextSlug":"/ai/5/","prevSlug":""}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}