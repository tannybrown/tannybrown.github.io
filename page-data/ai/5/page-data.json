{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/5/",
    "result": {"data":{"cur":{"id":"4e9354e9-a82a-5b0d-be7c-aa239d3ec41c","html":"<h2 id=\"0-회귀가-뭐야\" style=\"position:relative;\"><a href=\"#0-%ED%9A%8C%EA%B7%80%EA%B0%80-%EB%AD%90%EC%95%BC\" aria-label=\"0 회귀가 뭐야 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 회귀가 뭐야</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/215163818-4b204570-e3e1-42fd-a655-dfbcc48a5c64.png\" alt=\"image\"><br></p>\n<p><a href=\"https://tannybrown.github.io/ai/4/\">이전글</a>에서 인공신경망에 대해 알아보았다. <br>\r\n이번 글에서는 <strong>선형회귀</strong>에 대해 알아보자. <br>\r\n회귀는 ’<strong>입력과 출력간의 관계를 파악하는 것</strong>‘을 의미한다.<br></p>\n<blockquote>\n<p>회귀를 <strong>왜</strong> 배우나요?</p>\n</blockquote>\n<p>우린 회귀를 통해, <strong>처음보는 입력에 대해서 적절한 출력</strong>을 얻고 싶다.<br>\r\n그럼 선형 회귀는 뭘까?<br>\r\n어떤 <strong>연속적인 데이터 값</strong>들 간의 관계를 선형 함수로 모델링하는 것을 <strong>선형회귀(linear regression)</strong> 라 한다. <br></p>\n<blockquote>\n<p>연속적인 데이터가 뭔가요?</p>\n</blockquote>\n<p>데이터의 분류를 먼저 알아야하는데, 데이터는 크게 <strong>수치형</strong>과 <strong>범주형</strong>으로 나뉜다. <br>\r\n그리고 그 중 수치형은 <strong>연속형</strong>과 <strong>이산형</strong>으로 나뉜다. <br>\r\n예를 들어 몸무게, 키, 시간 등이 연속형에 속하며, 상품의 개수, 글자 수 등이 이산형에 속한다.</p>\n<blockquote>\n<p>??? : 선형이 뭔가요?</p>\n</blockquote>\n<p>선형이라함은, 쉽게 말해 <strong>일차식 관계</strong>를 의미한다. 선형의 의미에 대해서는 다른글에서 따로 다뤄보자. <br></p>\n<p>선형 회귀는 두 가지 유형으로 나눌 수 있다.</p>\n<ul>\n<li>\n<p>일반적인 선형 회귀(Simple Linear Regression) : 하나의 독립 변수(Independent variable)와 하나의 종속 변수(Dependent variable)로 구성된 데이터를 모델링하는 경우. <br>예를 들어, 공부 시간과 시험 점수 간의 관계를 모델링 할 때 사용한다.</p>\n</li>\n<li>\n<p>다중 선형 회귀(Multiple Linear Regression) : 여러 개의 독립 변수와 하나의 종속 변수로 구성된 데이터를 모델링하는 경우. <br>예를 들어, 공부 시간, 인터넷 사용 시간, 휴식 시간과 시험 점수 간의 관계를 모델링 할 때 사용한다.</p>\n</li>\n</ul>\n<p><br><br></p>\n<h2 id=\"1-선형회귀와-학습\" style=\"position:relative;\"><a href=\"#1-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%99%80-%ED%95%99%EC%8A%B5\" aria-label=\"1 선형회귀와 학습 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 선형회귀와 학습</h2>\n<p>예시를 하나 가져왔다. <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215274142-03f6ea1d-bcea-44f5-adee-0d32613b9aad.png\" alt=\"image\"><br>\r\n(reference : <a href=\"https://gilberttanner.com/blog/linear-regression-explained)\">https://gilberttanner.com/blog/linear-regression-explained)</a><br><br>\r\nx축이 공부시간, y축이 시험 점수이고 다음과 같은 data(점)들이 존재한다고 할때, x와 y의 선형관계를 빨간 직선으로 표현할 수 있다.<br>\r\n선형관계를 잘 표현한 적절한 빨간 직선을 찾는 것이 바로 <strong>학습</strong>이다.<br>\r\n위 직선은 다음과 같이 표현할 수 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215274972-af0916e9-f458-41df-945c-8c28b6e2c7a5.png\" alt=\"image\"><br>\r\ny위의 ^(hat)이 있는 이유는 <strong>실제 y값</strong>이 아닌 <strong>예측된 y값</strong>이라는 의미이다.<br>\r\n또한 앞선 글에서 배운 파라미터와 bias를 확인할 수 있다.<br>\r\n학습 방법은 매우 간단하다. 먼저 Loss(cost)를 정의한다. 이 경우 Loss로 MSE를 써보자.<br></p>\n<blockquote>\n<p>MSE는 mean square error의 약자로, (예측값과 실제 값의 차)의 제곱의 합을 의미한다.</p>\n</blockquote>\n<p>Loss는 이름에서 알 수 있듯, 작을 수록 좋다. 이 Loss의 값을 <strong>최소</strong>로 만드는 w와 b를 찾으면, 학습이 끝난다.<br>\r\n자 그러면, w와 b를 어떻게 찾을 수 있을까?<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215275914-56118e3d-78ed-4549-98ff-010e90810eef.png\" alt=\"image\"><br>\r\n<br>\r\n<strong>무지성 대입</strong>을 시도할 수도 있지만, 지성인으로서 스마트한 방법을 생각해보자.<br>\r\n우린 크게 두가지 방법을 사용할 수 있다.</p>\n<ul>\n<li><strong>Analytical method</strong></li>\n<li><strong>Numerical method</strong></li>\n</ul>\n<p>먼저 Analytic method는 수학적 접근이다. 직접 손으로 식을 써서 수학을 통해 문제를 해결한다. 반대로, Numerical method는 수치해석적 방식으로, 컴퓨터를 이용해 문제를 해결한다.<br>\r\n그리고 우리가 해결할 대부분의 경우에, Numerical method를 사용하게 될 것이다.</p>\n<blockquote>\n<p><strong>왜</strong> Analytical method는 많이 안쓰이나요?</p>\n</blockquote>\n<p>Analytical methods는 수학적 방법을 사용하여 문제를 해결하므로 문제가 수학적으로 표현될 수 있는 경우에만 사용할 수 있다. 하지만 많은 문제들이 수학적으로 표현하기 어렵거나, 계산 시간이 너무 오래 걸린다. 반면 Numerical method는 수학적으로 표현되지 않아도, 컴퓨터로 근사치를 구해낼 수 있다. <br>\r\n또한, 일부 문제는 실제 일어나는 현상을 모델링하는데, 이러한 모델은 수학적으로 정의가 어렵다. ex. 나비에 스토크스 <br>\r\n따라서 Numerical method가 더 많이 쓰인다.</p>\n<blockquote>\n<p>필자는 언제나 말하지만, 어떤 하나의 방법을 고수하는 것은 옳지 않다고 생각한다. 모든 방법엔 trade-off가 있고, <strong>주어진 상황에 알맞은 방법을 취사선택하는 것이 엔지니어가 가져야할 자세</strong>이다.</p>\n</blockquote>\n<p>Numerical method를 통해 w와 b의 값을 찾을 수 있다는 것까지 알았다.<br><br>\r\n다음글에서, 선형회귀에서 쓰이는 Numerical method, <strong>Gradient Descent</strong>에 대해 알아보자!</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%ED%9A%8C%EA%B7%80%EA%B0%80-%EB%AD%90%EC%95%BC\">0. 회귀가 뭐야</a></li>\n<li><a href=\"#1-%EC%84%A0%ED%98%95%ED%9A%8C%EA%B7%80%EC%99%80-%ED%95%99%EC%8A%B5\">1. 선형회귀와 학습</a></li>\n</ul>\n</div>","excerpt":"0. 회귀가 뭐야 image 이전글에서 인공신경망에 대해 알아보았다. \r\n이번 글에서는 선형회귀에 대해 알아보자. \r\n회귀는 ’입력과 출력간의 관계를 파악하는 것‘을 의미한다. 회귀를 왜 배우나요? 우린 회귀를 통해, 처음보는 입력에 대해서 적절한 출력을 얻고 싶다.\r\n그럼 선형 회귀는 뭘까?\r\n어떤 연속적인 데이터 값들 간의 관계를 선형 함수로 모델링하는 것을 선형회귀(linear regression) 라 한다.  연속적인 데이터가 뭔가요? 데이터의 분류를 먼저 알아야하는데, 데이터는 크게 수치형과 범주형으로 나뉜다. \r\n그리고 그 중 수치형은 연속형과 이산형으로 나뉜다. \r\n예를 들어 몸무게, 키, 시간 등이 연속형에 속하며, 상품의 개수, 글자 수 등이 이산형에 속한다. ??? : 선형이 뭔가요? 선형이라함은, 쉽게 말해 일차식 관계를 의미한다. 선형의 의미에 대해서는 다른글에서 따로 다뤄보자.  선형 회귀는 두 가지 유형으로 나눌 수 있다. 일반적인 선형 회귀(Simple …","frontmatter":{"date":"January 28, 2023","title":"4. 선형회귀를 하는 이유","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/5/"}},"next":{"id":"cfe83a47-50f7-5608-b434-5d7d2e9e3f00","html":"<h2 id=\"0-신경세포-뉴런\" style=\"position:relative;\"><a href=\"#0-%EC%8B%A0%EA%B2%BD%EC%84%B8%ED%8F%AC-%EB%89%B4%EB%9F%B0\" aria-label=\"0 신경세포 뉴런 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 신경세포 뉴런</h2>\n<p><a href=\"https://tannybrown.github.io/ai/3/\">이전글</a>에서 머신러닝의 분야에 대해 살펴보았다. <br>\r\n이번글에서는 딥러닝의 기초가 되는 인공신경망에 대해 다뤄보도록 하자.<br>\r\n<br>\r\n당신이 고등학교 생명과학 시간에 졸지 않았다면 이 그림을 기억할 것이다.<br><br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215143541-5dff4459-54d7-4dbb-bdd5-fd6bb3b4e511.png\" alt=\"image\"><br>\r\n<br>\r\n위 그림은 신경세포 <strong>뉴런</strong>을 나타낸 것으로, 가지돌기에서 여러 자극을 전달받고, 핵에서 자극을 전달할지를 결정하며, 축삭돌기를 통해 또 다른 신경세포로 자극이 전달된다.<br>\r\n딥러닝에서 사용되는 <strong>인공신경망</strong>은 이름에서 알수 있듯, 이 신경세포의 동작방식과 닮아 있다. <br><br><br></p>\n<h2 id=\"1-퍼셉트론\" style=\"position:relative;\"><a href=\"#1-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0\" aria-label=\"1 퍼셉트론 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 퍼셉트론</h2>\n<p>우리의 신체에 뉴런이 있듯, 딥러닝의 인공신경망에는 <strong>퍼셉트론</strong>이 있다.\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215151104-d731ca4b-ca65-4651-bb82-1e82b4ca5ed6.png\" alt=\"image\"><br>\r\n위 그림은 퍼셉트론을 나타낸 것인데 한번 살펴보자.<br>\r\n우선 좌측의 x 값들은 퍼센트론의 <strong>input</strong>에 해당한다. 위 예시에서는 두개만 그렸지만 여러개의 <strong>input</strong>이 있을 수 있다.<br>\r\n그리고 x와 연결된 선에 써있는 w를 우린 <strong>가중치</strong> 혹은 <strong>파라미터</strong> 라고 부른다. 이 파라미터는 각각의 input들과 곱해진다.<br>\r\n그리고 가장 아래 있는 <strong>bias</strong>는 <strong>편향, 민감도</strong>를 나타내며 따로 가중치가 곱해지지 않는다. <br>\r\n퍼셉트론은 이렇게 들어온 인풋과 파라미터를 곱한 값을 더하고 바이어스까지 더해준다. 수식으로 표현하면 다음과 같다.<br><br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215153563-403ed017-a9f1-4444-bf4c-27836bcefbb0.png\" alt=\"image\"><br>\r\n<br>\r\n이렇게 계산하면 끝이냐? 아니다. 이렇게 계산한 결과를 <strong>활성화 함수(activation function)</strong> 에 넣어준다.<br>\r\n이때 활성화 함수로는 <strong>unit step function</strong>(입력이 0보다 작으면 0, 1보다 크면 1을 출력)을 사용한다.<br></p>\n<blockquote>\n<p>자 이제 슬슬 궁금한게 있을 것이다. 하나씩 대답해주겠다.</p>\n</blockquote>\n<p><strong>??? : 바이어스는 왜 있어야하나요?</strong><br>\r\n바이어스가 없다면, input으로 0이 들어왔을때 언제나 출력값이 0이 된다. 즉슨, 인풋이 0일때 우리가 원하는 결과를 맞춰주기 위해 필요하다. <br><br>\r\n<strong>??? : 활성화 함수는 unit step function만 쓰나요?</strong> <br>\r\n퍼셉트론에서는 unit step function을 사용한다. 하지만 MLP(Multi-Layer perceptron)등 다른 모델에서는 여러가지 다양한 활성화 함수를 사용한다. <br></p>\n<blockquote>\n<p>추가적으로, 퍼셉트론의 선을 edge라고 부르고, activation 함수가 있는 동그라미를 node라고 부른다. 알아두자.</p>\n</blockquote>\n<p><br><br></p>\n<h2 id=\"2-다층-퍼셉트론mlp\" style=\"position:relative;\"><a href=\"#2-%EB%8B%A4%EC%B8%B5-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0mlp\" aria-label=\"2 다층 퍼셉트론mlp permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 다층 퍼셉트론(MLP)</h2>\n<p>앞서 살펴본 퍼셉트론을 <strong>‘단층 퍼셉트론’</strong> 이라고 부른다. <br>\r\n말그대로 층이 하나였기 때문에 붙여진 이름인데, 우린 이러한 단층 퍼셉트론을 여러겹으로 쌓을 수 있다. 그렇게 여러층의 퍼셉트론으로 구성한 것을 ’<strong>다층 퍼셉트론</strong>‘이라고 부른다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/215158159-dd434850-9cc3-4436-b371-1809b1133dea.png\" alt=\"image\"><br></p>\n<p>몇가지 용어를 더 살펴보면, 입력을 받는 부분을 <strong>입력층</strong>, 출력하는 부분을 <strong>출력층</strong> 이라고 하며, 중간에 있는 layer를 <strong>은닉층(hidden layer)</strong> 라고 한다. 그리고 모든 노드에 엣지가 연결되어 있으면 <strong>fully connected</strong> 라고 부른다. <br>\r\n마지막으로, 은닉층이 2개 이상인 신경망을 우린 <strong>DNN(Deep Neural Network)</strong> 이라고 한다.</p>\n<blockquote>\n<p>DNN? 어디서 들어봤는데?</p>\n</blockquote>\n<p>라는 생각이 들었다면 정말 고맙다. 맞다. 첫번째 글에서 살펴보았던, 딥러닝에 사용된다고 소개했던 것이 바로 DNN이다.<br></p>\n<p><br><br></p>\n<h2 id=\"3-그래서-이걸-어떻게-써먹어요\" style=\"position:relative;\"><a href=\"#3-%EA%B7%B8%EB%9E%98%EC%84%9C-%EC%9D%B4%EA%B1%B8-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%8D%A8%EB%A8%B9%EC%96%B4%EC%9A%94\" aria-label=\"3 그래서 이걸 어떻게 써먹어요 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 그래서 이걸 어떻게 써먹어요?</h2>\n<p>지금까지 잘 따라왔다면 이런 의문이 든다. <br></p>\n<blockquote>\n<p>퍼셉트론, 다층 퍼셉트론… 일단 뭔진 알겠는데 이거 가지고 뭐 어떻게 머신러닝, 딥러닝을 한다는거야?</p>\n</blockquote>\n<p><strong>학습</strong>에 대해 설명하면 이 궁금증은 해결된다. <br>\r\n머신러닝, 딥러닝에서 말하는 <strong>학습</strong>이라는 것이 바로, 위에서 살펴본 <strong>가중치</strong>와 <strong>bias</strong>를 업데이트하며 <strong>최적의 가중치와 bias</strong>를 찾는 과정이다. <br>\r\n뭔소린지 모르겠다고?</p>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/215161222-9a6ad1d3-df29-4f6b-9be0-3127625a06ce.png\" alt=\"image\"><br></p>\n<p>다음글에서 이에대한 궁금증을 해결해보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%8B%A0%EA%B2%BD%EC%84%B8%ED%8F%AC-%EB%89%B4%EB%9F%B0\">0. 신경세포 뉴런</a></li>\n<li><a href=\"#1-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0\">1. 퍼셉트론</a></li>\n<li><a href=\"#2-%EB%8B%A4%EC%B8%B5-%ED%8D%BC%EC%85%89%ED%8A%B8%EB%A1%A0mlp\">2. 다층 퍼셉트론(MLP)</a></li>\n<li><a href=\"#3-%EA%B7%B8%EB%9E%98%EC%84%9C-%EC%9D%B4%EA%B1%B8-%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%8D%A8%EB%A8%B9%EC%96%B4%EC%9A%94\">3. 그래서 이걸 어떻게 써먹어요?</a></li>\n</ul>\n</div>","frontmatter":{"date":"January 28, 2023","title":"3. 딥러닝의 시작, 인공신경망","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/4/"}},"prev":{"id":"e9cd9f14-c115-5b0b-a667-0d8058e4739a","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/216059096-c5eeb283-5e96-42f6-bd15-7c64ec2b3137.png\" alt=\"image\">\r\n<br>\r\n<a href=\"https://tannybrown.github.io/ai/5/\">이전글</a>에서 선형회귀와 Loss등에 대해 알아보았다. <br>\r\n그리고 우린, 스마트한 방법으로 optimal한 파라미터와 bias를 찾기 위해 Numerical Method를 사용한다는 것까지 알아봤다. <br>\r\n그리고 오늘은 Numerical Method인 Gradient Descent에 대해 알아보자.</p>\n<p><br><br></p>\n<h2 id=\"1-직관적-이해\" style=\"position:relative;\"><a href=\"#1-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4\" aria-label=\"1 직관적 이해 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 직관적 이해</h2>\n<p>Gradient Descent는 직역하면, 경사하강법이다.<br> 이 방법은 <strong>Loss가 줄어드는 방향</strong>을 찾아 파라미터와 bias를 업데이트 하는 방법이라고 할 수 있다.<br>\r\n예시를 통한 직관적 이해를 해보자.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216073365-d46b5057-690c-4186-b7b8-9727fefb9778.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>그림퀄리티가 상당하다 <br></p>\n</blockquote>\n<p>자, 만약 위 그림에서 최저점을 찾고싶다고 하자. 그리고 현재 우리의 위치가 <strong>파란색 점</strong>으로 주어졌다고 한다면, 우린 어떤 방향으로 나아가야할까?<br>\r\n여러가지 방법론이 있을 수 있지만, <strong>Gradient Descent</strong>에서는 <strong>현재위치에서 기울기의 반대방향</strong> 으로 나아간다.<br>\r\n즉, 기울기가 양수(+)이므로 (-)방향으로 나아가야한다.<br>\r\n자 그럼 어디갈지 알았으니 한발짝 나가아보자. <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216073715-42b5ea14-97f4-426c-af86-c8782d1b95e6.png\" alt=\"image\">\r\n<br>\r\n엇 이번엔 기울기의 부호가 달라졌다. 현재 기울기의 부호는 (-)이므로 (+)방향으로 가야한다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216074025-07c7dabd-f67f-4f6c-a464-2e824428d45f.png\" alt=\"image\"><br>\r\n드디어 도착했다. 이렇게 기울기가 0이 되면 움직임을 멈추고 최저점을 찾아냈다는 것을 알 수 있다.<br></p>\n<p><br><br>\r\n다소 야매로 그린 예시였지만, 위 그래프가 Loss에 대한 그래프라고 생각한다면, loss가 가장 작아지는 값을 찾고 싶은 우리가 원하는 값에 도달할 수 있었던 것이다.<br>\r\n위 매커니즘을 식으로 표현하면 다음과 같다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216074281-39220e6e-6ced-42ef-bbdf-d43c76437bc3.png\" alt=\"image\"><br>\r\nw는 가중치(파라미터)를 나타내고, L은 Loss function을 의미한다. 입실론은 학습률(learning rate)를 나타내는데, 이는 step-size라고 하기도 한다. 이 step-size에 따라 w를 얼마나 이동시킬지가 달라지며, step-size를 <strong>상수(고정값으로)</strong> 으로 둘 수도 있고 <strong>스케줄링</strong>해서 변하는 값으로 둘 수도 있다.<br></p>\n<blockquote>\n<p>bias는요?</p>\n</blockquote>\n<p>bias도 결국 parameter이다. 즉, 같은 방식인데 bias 에 대해서 같은 과정을 수행하면 된다. 다만 w따로, bias 따로 계산해주는게 귀찮을 수 있다. 따라서 homogeneous equation으로 바꿔서 계산한다면(w의 차원을 하나 올려서) 계산의 편의를 가져갈 수 있다.<br><br></p>\n<h2 id=\"2-한계\" style=\"position:relative;\"><a href=\"#2-%ED%95%9C%EA%B3%84\" aria-label=\"2 한계 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 한계</h2>\n<p>위 글을 따라 오면서 눈치챈 사람이 있을수도 있겠다. Gradient Descent는 명확한 한계를 가진다. <br></p>\n<ul>\n<li>너무 신중하다.</li>\n<li>local minimum <br></li>\n</ul>\n<p>우선 너무 신중하다. 즉 한번의 update를 위해서 갖고 있는 모든 데이터를 이용해 Loss를 계산해야한다. 데이터수가 많아지면 매우매우 느려진다.<br>\r\n두번째로 Local Minimum을 찾는 알고리즘이라는 것이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/216072186-48307436-a632-4a7a-9a5d-7eeaa6b781a5.png\" alt=\"image\"><br></p>\n<p>우리가 알고싶은 것은 global minimum이다. 만약 위와 같은 loss가 주어진다면, 원하는 최저값에 도달하지 못할 수 있다.(<strong>즉, 초기값에 따라 결과가 달라질 것이다</strong>.)<br>\r\n즉, convex한 문제에서만 적용이 가능하다.</p>\n<blockquote>\n<p>알고리즘 공부를 한사람이라면, 눈치챘을 것이다. Gradient Descent는 greedy 하다.</p>\n</blockquote>\n<p><br><br></p>\n<h2 id=\"3-마무리\" style=\"position:relative;\"><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"3 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 마무리</h2>\n<p>이번글에서는 gradient descent에 대해 알아보았다.<br>\r\n다음글에서는 gradient descent의 보완된 버전들에 대해 알아보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-%EC%A7%81%EA%B4%80%EC%A0%81-%EC%9D%B4%ED%95%B4\">1. 직관적 이해</a></li>\n<li><a href=\"#2-%ED%95%9C%EA%B3%84\">2. 한계</a></li>\n<li><a href=\"#3-%EB%A7%88%EB%AC%B4%EB%A6%AC\">3. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"January 29, 2023","title":"5. Gradient Descent 하급편","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/6/"}},"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/5/","nextSlug":"/ai/4/","prevSlug":"/ai/6/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}