{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/18/",
    "result": {"data":{"cur":{"id":"168da2c2-0f3e-5f80-8b28-7efc66e0d9be","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/17/\">지난글</a>에서 vanishing gradient의 해결법에 대해 살펴보았다.<br>\r\n이번글에서는 나머지 문제인 overfitting을 해결하는 방법에 대해 이야기 해보자.<br></p>","excerpt":"0. 지난 이야기 지난글에서 vanishing gradient의 해결법에 대해 살펴보았다.\r\n이번글에서는 나머지 문제인 overfitting을 해결하는 방법에 대해 이야기 해보자.","frontmatter":{"date":"February 16, 2023","title":"17. 깊은 인공신경망의 문제 3편(feat. overfitting)","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/18/"}},"next":{"id":"c41816b3-faaa-5298-8c4a-b79d954d018d","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p>깊은 인공신경망의 문제점 중 하나인 vanishing gradient…<br>\r\n이를 해결하기 위한 방안인 <strong>ReLU</strong>에 대해 살펴보았다.<br>\r\n이번글에서는 또 다른 방안인 <strong>Batch Normalization</strong>에 대해 살펴보겠다.<br></p>\n<br>\r\n<br>\n<h2 id=\"1-batch-normalization\" style=\"position:relative;\"><a href=\"#1-batch-normalization\" aria-label=\"1 batch normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Batch Normalization</h2>\n<p>자 <a href=\"https://tannybrown.github.io/ai/16/\">이전시간</a>에 배운 ReLU를 이용하여 신경망을 구성한다고 해보자.<br>\r\nBatch Normalization의 batch는 mini-batch 알고리즘의 batch를 의미한다.<br>\r\nbatch normalization은 어떤 layer에 적용할지를 선택한다. (이는 하이퍼 파라미터이다 ㅇㅇ)<br>\r\n예시 그림에서는 첫번째 보이는 hidden layer에 batch normalization을 적용하려 한다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219054999-f17a5ee4-ded4-411d-848e-c128e7ebe60e.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>예시니까 간단한 구조로 그린것임</p>\n</blockquote>\n<p>자 이때 ReLU에 집중해보자.<br>\r\n만약 ReLU에 들어온 데이터가 모두 양수였다면, 모두 linear하게 나갈것이다.<br>\r\n그렇게 되면 이 액티베이션이 ReLU인지 linear activation인지 분간할 수가 없어진다. 즉 not linearity를 살려주지 못한다.<br>\r\n이를 방지하기 위해서(<strong>= not linearity를 살려주기 위해서</strong>) 우리가 정한 layer의 batch를 normalization해줄 것이다.<br>\r\n다음과 같이 말이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219058965-23502c51-059a-40c0-8363-5476b6cee86b.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>그러면 어떻게 Normalization을 하냐?\r\n걍 0근처로 옮기는 건가요?</p>\n</blockquote>\n<p>우선 위 그림과 같이 normalization이 되기 위해서는 <strong>평균</strong>과 <strong>표준편차</strong>가 필요하다.(정규화라는 작업 자체가 그렇다 ㅇㅇ)<br>\r\n그럼 평균과 표준편차를 우리가 정하는 것이냐?<br>\r\n<strong>아니다.</strong> batch normalization에서는 적절한 평균과 표준편차를 머신이 학습을 통해서 알아낸다.</p>\n<blockquote>\n<p>parametric ReLU가 기울기를 학습하는 방식과 같다.</p>\n</blockquote>\n<p>이러한 방식으로 vanishing gradient를 해결하는 방식을 batch normalization 이라고 한다.\r\n<br>\r\n<br></p>\n<h2 id=\"2-layer-normalization\" style=\"position:relative;\"><a href=\"#2-layer-normalization\" aria-label=\"2 layer normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Layer Normalization</h2>\n<p>Batch Normalization말고도 Layer Normalization이라는 방식 또한 존재한다.<br>\r\nLayer Normalization은 batch와는 관련이 없고, 각 레이어에서의 평균과 분산을 사용하여 각 입력 데이터의 정규화를 수행한다.<br>\r\n두 정규화에 대해 자세하게 다루진 않을 것이다. 우선은 이러한 정규화 기법들이 존재한다 정도를 기억하자.</p>\n<blockquote>\n<p>참고로, batch normalization은 배치 사이즈가 작으면 쓰기가 어렵다. 하지만 layer normalization은 배치사이즈와 무관하게 적용이 가능하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"3-그럼-해결된건가\" style=\"position:relative;\"><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\" aria-label=\"3 그럼 해결된건가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 그럼 해결된건가?</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219062495-831f07b4-2c5c-4a46-8760-64e755619dfa.png\" alt=\"image\"><br>\r\n안타깝게도 아니다. ReLU와 Batch Normalization을 적용하더라도 Loss의 개형이 복잡해져서 학습이 잘 일어나지 않을 수 있다.<br></p>\n<blockquote>\n<p>그러니 만병통치약은 없다. 상황에따라서 적절히 기법들을 적용하는 것이 중요하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"4-마무리\" style=\"position:relative;\"><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"4 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 마무리</h2>\n<p>이번글에서는 batch normalization에 대해 알아보았다.<br>\r\n다음글에서는 overfitting문제의 해결 기법에 대해 알아보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-batch-normalization\">1. Batch Normalization</a></li>\n<li><a href=\"#2-layer-normalization\">2. Layer Normalization</a></li>\n<li><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\">3. 그럼 해결된건가?</a></li>\n<li><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\">4. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"February 16, 2023","title":"16. 깊은 인공신경망의 문제 2편","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/17/"}},"prev":null,"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/18/","nextSlug":"/ai/17/","prevSlug":""}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}