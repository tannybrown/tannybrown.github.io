{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/15/",
    "result": {"data":{"cur":{"id":"f5da3d83-113e-5df7-910c-14c6eb9e604b","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/218723923-26e429a5-1d3c-4c95-94aa-ac197c3bcd28.png\" alt=\"image\"><br>\r\n<a href=\"https://tannybrown.github.io/ai/14/\">이전글</a>에서 다중분류에 대해 알아보았다.<br>\r\n이번글에서는 인공신경망이 이렇게까지 쓰이는 이유가 뭘지 한번 알아보자.<br><br></p>\n<h2 id=\"1-인공신경망을-쓰는-과정\" style=\"position:relative;\"><a href=\"#1-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EC%93%B0%EB%8A%94-%EA%B3%BC%EC%A0%95\" aria-label=\"1 인공신경망을 쓰는 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 인공신경망을 쓰는 과정</h2>\n<p>지금까지 공부한 내용을 summary 해보자.\r\n인공신경망을 이용하는 큰 4가지 step에 대해 공부해보았다.</p>\n<h4 id=\"step-1-입출력-정의\" style=\"position:relative;\"><a href=\"#step-1-%EC%9E%85%EC%B6%9C%EB%A0%A5-%EC%A0%95%EC%9D%98\" aria-label=\"step 1 입출력 정의 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 1. 입출력 정의</h4>\n<p>입출력을 정의해야했다. 입력을 독립변수, 출력을 종속변수라고 했으며, 출력이 여러개일수도 한개일수도 있었다.<br></p>\n<h4 id=\"step-2-모델-만들기\" style=\"position:relative;\"><a href=\"#step-2-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0\" aria-label=\"step 2 모델 만들기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 2. 모델 만들기</h4>\n<p>모델을 만들어야했다. 입력층과 출력층, 그리고 가장중요한 hidden layer의 구성.<br>\r\nhidden layer를 어떻게 디자인 하는가가 모델의 성능을 결정한다. 하지만 이에 대해서는 자세히 아직 다루지 않았고, CNN을 공부하며 공부해볼 것이다.<br></p>\n<h4 id=\"step-3-loss-정의\" style=\"position:relative;\"><a href=\"#step-3-loss-%EC%A0%95%EC%9D%98\" aria-label=\"step 3 loss 정의 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 3. loss 정의</h4>\n<p>모델까지 만들었으면 Loss를 정의해야했다. 주어진 입출력과 activation을 고려해서 Loss를 정의했고 지금까지 배운 loss는 MSE, log likelihood, cross-entropy가 있었다.<br></p>\n<h4 id=\"step-4-weight-bias-찾기\" style=\"position:relative;\"><a href=\"#step-4-weight-bias-%EC%B0%BE%EA%B8%B0\" aria-label=\"step 4 weight bias 찾기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 4. weight, bias 찾기</h4>\n<p>Loss 정의까지 마쳤다면, 이제 학습을 시켜야한다. 즉 최적의 weigth 와 bias를 찾는 과정이다. <br>\r\n이에 관해서는 gradient descent, SGD, mini-batch, adam 등을 공부했다. 새로운 알고리즘을 개발하는 것도 좋은 연구분야일 것이다.</p>\n<p><br><br></p>\n<h2 id=\"2-인공신경망을-굳이-써야하나\" style=\"position:relative;\"><a href=\"#2-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EA%B5%B3%EC%9D%B4-%EC%8D%A8%EC%95%BC%ED%95%98%EB%82%98\" aria-label=\"2 인공신경망을 굳이 써야하나 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 인공신경망을 굳이 써야하나?</h2>\n<p>결론적으로 모델이란건 <strong>함수</strong>였고, 인공신경망을 통해서 그 함수를 구현했다.<br>\r\n그런데 <strong>왜</strong> 굳이 인공신경망이라는 구조를 이용한걸까?<br>\r\n그 이유는 바로, 인공신경망을 이용하면 <strong>모든 연속 함수를 근사할 수 있기 때문</strong>이다.<br>\r\n이를 <strong>Universal Approximation Theorem</strong>이라고 한다.</p>\n<p><br><br></p>\n<h2 id=\"3-universal-approximation-theorem\" style=\"position:relative;\"><a href=\"#3-universal-approximation-theorem\" aria-label=\"3 universal approximation theorem permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Universal Approximation Theorem</h2>\n<p>Universal Approximation Theorem은 인공신경망의 근간이 되는 이론으로, 하나의 hiddle layer로 모든 연속 함수를 근사할 수 있다는 것인데, 예시를 살펴보며 이해해보자.<br>\r\n여기 키와 몸무게에 대한 데이터가 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218725303-6cf633d0-db44-4e96-94d3-22548a45a15b.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>등간격이 아닌점들은 양해를 바란다. 그냥 수치만 보자.</p>\n</blockquote>\n<p>키는 <strong>독립변수</strong>, 몸무게가 <strong>종속변수</strong>라고 해보자. 즉, 키가 주어졌을때 적절한 몸무게를 예측 하고 싶다.<br>\r\n이렇게 데이터가 주어졌다면 우린 이 데이터들을 만족하는 함수를 찾고 싶다.<br>\r\n이때 이용하는 전략은 각 데이터당 2개의 노드를 만들고, 그 두 노드를 이용해 loss가 0이 되도록 하는 함수를 만들어 보도록 하겠다.<br>\r\n우선은 가장 첫번째로, (키 : 150, 몸무게 : 70)데이터를 보자.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218773417-42232304-f199-4ade-b139-c664f0955675.png\" alt=\"image\"><br>\r\n두개의 노드를 이용해서 히든레이어를 구성했고 각각은 unit step function을 activation으로 갖고 있다.<br>\r\n이러한 경우에 출력은 이러한 폼이 된다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218774304-59cd9632-c24c-4e98-ae3f-e324694949e1.png\" alt=\"image\"><br>\r\n149~151사이의 데이터만 70의 출력을 갖는 개형이다.<br>\r\n이러한 짓을 모든 데이터에 대해 각각 한다면??<br>\r\n아마 주어진 데이터들에 모두 이 과정을 수행하면 다음과 같은 신경망이 그려질 것이다.<br></p>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/218775975-6e3668f2-3179-4024-96ec-0b192c8562c3.png\" alt=\"image\">\r\n<br>\r\n그리고 이 신경망의 출력을 그래프로 표현하면,<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218776742-b500425c-b3d7-4ca1-8b4c-620110af19cb.png\" alt=\"image\"><br></p>\n<p>이와 같음을 알 수 있다.<br>\r\n더많은 데이터가 주어져도 마찬가지의 방법을 통한다면, Loss가 0인 fitting하는 연속함수를 만들 수 있음을 알 수 있다.<br></p>\n<blockquote>\n<p>이게 뭐야 그냥 어거지 아니냐?<br>\r\n그러한 생각을 할 수 있지만, ‘위의 예시는 이렇게 해서 만들 수 있다.‘를 설명하기 위한것이지. 이렇게 만들거다 라는 것은 아니다.<br>\r\n즉, 어떠한 데이터가 주어져도 그에 맞는 함수를 만들 수 있음을 보인것이다.</p>\n</blockquote>\n<p><br><br></p>\n<h2 id=\"4-마무리\" style=\"position:relative;\"><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"4 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/218778227-c093d8c5-fc11-4dd6-8480-5a0a35adb14b.png\" alt=\"image\"><br></p>\n<p>universal approximation theorem을 이용하면, 어떤 연속함수든 근사해낼 수 있었다.<br>\r\n이러한 정리를 바탕으로 인공신경망을 통해서 우리는 (overfitting이 일어나지 않게)잘 학습만 시키면 된다는 것을 알 수 있다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/16/\">다음글</a>에서는 인공신경망의 문제점들에 대한 이야기를 다뤄보겠다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></p>\n</li>\n<li>\n<p><a href=\"#1-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EC%93%B0%EB%8A%94-%EA%B3%BC%EC%A0%95\">1. 인공신경망을 쓰는 과정</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"#step-1-%EC%9E%85%EC%B6%9C%EB%A0%A5-%EC%A0%95%EC%9D%98\">step 1. 입출력 정의</a></li>\n<li><a href=\"#step-2-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0\">step 2. 모델 만들기</a></li>\n<li><a href=\"#step-3-loss-%EC%A0%95%EC%9D%98\">step 3. loss 정의</a></li>\n<li><a href=\"#step-4-weight-bias-%EC%B0%BE%EA%B8%B0\">step 4. weight, bias 찾기</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EA%B5%B3%EC%9D%B4-%EC%8D%A8%EC%95%BC%ED%95%98%EB%82%98\">2. 인공신경망을 굳이 써야하나?</a></p>\n</li>\n<li>\n<p><a href=\"#3-universal-approximation-theorem\">3. Universal Approximation Theorem</a></p>\n</li>\n<li>\n<p><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\">4. 마무리</a></p>\n</li>\n</ul>\n</div>","excerpt":"0. 지난 이야기 image\r\n이전글에서 다중분류에 대해 알아보았다.\r\n이번글에서는 인공신경망이 이렇게까지 쓰이는 이유가 뭘지 한번 알아보자. 1. 인공신경망을 쓰는 과정 지금까지 공부한 내용을 summary 해보자.\r\n인공신경망을 이용하는 큰 4가지 step에 대해 공부해보았다. step 1. 입출력 정의 입출력을 정의해야했다. 입력을 독립변수, 출력을 종속변수라고 했으며, 출력이 여러개일수도 한개일수도 있었다. step 2. 모델 만들기 모델을 만들어야했다. 입력층과 출력층, 그리고 가장중요한 hidden layer의 구성.\r\nhidden layer를 어떻게 디자인 하는가가 모델의 성능을 결정한다. 하지만 이에 대해서는 자세히 아직 다루지 않았고, CNN을 공부하며 공부해볼 것이다. step 3. loss 정의 모델까지 만들었으면 Loss를 정의해야했다. 주어진 입출력과 activation을 고려해서 Loss를 정의했고 지금까지 배운 loss는 MSE, log likelihoo…","frontmatter":{"date":"February 14, 2023","title":"14. 인공신경망이 쓰이는 이유","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/15/"}},"next":{"id":"48277e64-1a35-5e44-8616-9512ac7d3a33","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/13/\">이전글</a>에서 이진분류에 대해 다루었고,<br>\r\n이번글에서는 다중분류에 대해 다루려고 한다.<br></p>\n<br>\r\n<br>\n<h2 id=\"1-출력은-몇개로-할까\" style=\"position:relative;\"><a href=\"#1-%EC%B6%9C%EB%A0%A5%EC%9D%80-%EB%AA%87%EA%B0%9C%EB%A1%9C-%ED%95%A0%EA%B9%8C\" aria-label=\"1 출력은 몇개로 할까 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 출력은 몇개로 할까?</h2>\n<p>이진분류에서는 출력을 하나로 잡았다. sigmoid 를 이용해서 중간을 기점으로 이진분류를 진행했다.<br>\r\n그렇다면 다중분류에서도 마찬가지로 출력을 하나로 둬도 될까?<br>\r\nNoop! 안된다. 왜 안될까?</p>\n<blockquote>\n<p>언제나 reasoning이 중요하다. 왜? 왜? 왜?를 붙여보자.</p>\n</blockquote>\n<h3 id=\"1-불공평한-backpropagation\" style=\"position:relative;\"><a href=\"#1-%EB%B6%88%EA%B3%B5%ED%8F%89%ED%95%9C-backpropagation\" aria-label=\"1 불공평한 backpropagation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(1) 불공평한 backpropagation</h3>\n<p>불공평? 무슨뜻인가 하니, 예를 들어 삼중분류를 한다고 가정해보자. <br>그렇다면 마지막 출력층의 sigmoid의 영역을 설정하여 3개의 class에 매핑을 시켜야 할것이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218678225-edc36681-f12d-4d04-bf56-9e123016dc78.png\" alt=\"image\"><br>\r\n위 예는 A클래스는 0, B클래스는 1/2, C클래스는 1로 잡은 예이다.<br>\r\n이렇게 될 경우, B클래스는 항상 큰 gradient를 갖게 되고 A와 C클래스는 상대적으로 작은 Gradient를 갖게 되어 학습이 잘 이뤄지지 않을 것이다.</p>\n<blockquote>\n<p>이진분류에서는 왜 가능했나요?<br>\r\n이진분류때는 정답을 0과 1로 나누었고, (0,1/2)을 기점으로 점대칭이었기때문에 가능했다.</p>\n</blockquote>\n<h3 id=\"2-분류-가짓수가-많아지면\" style=\"position:relative;\"><a href=\"#2-%EB%B6%84%EB%A5%98-%EA%B0%80%EC%A7%93%EC%88%98%EA%B0%80-%EB%A7%8E%EC%95%84%EC%A7%80%EB%A9%B4\" aria-label=\"2 분류 가짓수가 많아지면 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(2) 분류 가짓수가 많아지면..?</h3>\n<p>분류가짓수가 1000개라면? sigmoid를 1000개로 쪼갤것인가? 이러한 경우 아주 약간의 가중치 변화로도 다른 class로 분류되며 오류가 커질 것이다.\r\n<br></p>\n<h3 id=\"3-이상한-사전-정보\" style=\"position:relative;\"><a href=\"#3-%EC%9D%B4%EC%83%81%ED%95%9C-%EC%82%AC%EC%A0%84-%EC%A0%95%EB%B3%B4\" aria-label=\"3 이상한 사전 정보 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>(3) 이상한 사전 정보</h3>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/218679849-941e63b6-9386-4b9f-83db-00903615d746.png\" alt=\"image\">\r\n<br>\r\n위 ABC클래스를 각각 소 고양이 강아지라고 해보자. <br>\r\n그러면 우린 의도하지 않았지만, 소와 고양이는 가깝고 소와 강아지는 멀다는 사전정보를 주게 된다.<br>\r\n그리고 소와 강아지 중간은 고양이라는 이상한 정보 또한 주어지게 된 것이다.<br></p>\n<blockquote>\n<p>즉 각 클래스별로의 관련성은 우리가 고려했던 정보는 아닌데, 의도치 않게 주게 된 것.</p>\n</blockquote>\n<p>그래서 소처럼 생긴 강아지가 input으로 들어온다면, 고양이라고 해버리는 경우가 생길 수 있는 것이다.<br></p>\n<br>\r\n<br>\n<h2 id=\"2-n개로-하자\" style=\"position:relative;\"><a href=\"#2-n%EA%B0%9C%EB%A1%9C-%ED%95%98%EC%9E%90\" aria-label=\"2 n개로 하자 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. n개로 하자!</h2>\n<p>출력을 1개로 두자니, 위와 같은 문제들이 발생한다. 따라서 이러한 문제들을 해결하기 위해서 출력을 n개로 두는 방법을 택했다.<br>\r\nn은 클래스의 수이다. 5개로 분류한다면 5, 10개로 분류한다면 10.<br>\r\n그러면 정답은 어떻게 판별할까? 이는 one-hot encoding을 이용한다.<br>\r\n예를 들어서, 소 고양이 강아지를 분류하는 모델이라면<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218682429-4a0a1f43-75dc-4672-bb57-a26c05dbd334.png\" alt=\"image\">\r\n<br>\r\n이렇게 각 클래스를 담당하는 출력 노드가 존재하고, 1 0 0 , 0 1 0, 0 0 1 과 같은 출력으로 각 클래스를 구분한다.<br></p>\n<blockquote>\n<p>1 0 0 , 0 1 0, 0 0 1 과 같이 정답만 1 나머지는 0 으로 표현하여 각 클래스를 구분하는 방법을 one-hot encoding이라고 한다.</p>\n</blockquote>\n<p>이제 1 0 0 , 0 1 0 , 0 0 1 의 출력이 나오도록 학습을 시키는 것이다.<br>\r\n이때 이 출력의 결과를 이진분류때와 마찬가지로 확률분포로 해석한다.<br>\r\n이게 무슨 말인가 하니, 이진분류의 경우 강아지와 고양이를 분류한다고 하면,<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218689410-76c0e22a-8edf-46b8-b466-3b1dc7b52897.png\" alt=\"image\">\r\n<br>\r\n다음과 같은 출력을 기대하는 것을 의미한다.<br>\r\n베르누이 분포로 해석이 가능하며 출력의 결과를 확률분포로 보자는 것이다.</p>\n<blockquote>\n<p>다소 내용이 안와닿을 것 같다. 이에 관해서는 추후에 따로 글을 올리도록 하겠다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"3-activation-functionsoftmax\" style=\"position:relative;\"><a href=\"#3-activation-functionsoftmax\" aria-label=\"3 activation functionsoftmax permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Activation function(softmax)</h2>\n<p>자 그러면 우리는 어떤 activation을 이용할까?<br>\r\n새로운 activation이 등장하게 되는데, softmax를 사용하게 된다.\r\nsoftmax는 <strong>인공신경망으로 표현이 안되는 activation</strong>으로 마지막에 linear activation을 거친후 softmax를 거치는 방식으로 그릴 수 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218690201-a549fcf5-7a02-4d77-a39d-490fcddb935b.png\" alt=\"image\">\r\n<br>\r\nx1,x2,x3를 입력으로 받아서 softmax를 거치면 q1,q2,q3의 값이 나온다. 이때 q1,q2,q3는 <strong>확률분포</strong>이므로 <strong>셋의 합은 1</strong>, <strong>각 값은 0~1</strong>이라는 조건을 만족해야 한다.<br></p>\n<p>그러면 각각의 q값은 어떻게 출력이 될까?다음과 같이 표현된다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218691495-ec1849c6-5b7b-401e-a08c-3d2c30ddb102.png\" alt=\"image\"><br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218691576-6002f48d-e0b8-4c95-8e48-83e87460ef82.png\" alt=\"image\"><br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218691650-bff9c450-667d-4641-9008-ea760a3e0b59.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>꼭 저렇게 표현되어야 하는가? 아니다. 하지만 많이 사용되는 방식이고, 여러 표현에 대한 관점들은 다른 글을 통해 또 이야기 해보자.</p>\n</blockquote>\n<h2 id=\"4-loss-정의\" style=\"position:relative;\"><a href=\"#4-loss-%EC%A0%95%EC%9D%98\" aria-label=\"4 loss 정의 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. Loss 정의</h2>\n<p>softmax를 이용한 다중분류에서는 Loss함수로 Cross-entropy가 쓰인다.</p>\n<blockquote>\n<p>크로스 엔트로피는 뭐냐?<br>\r\n두 확률분포의 거리를 표현하는 식이라고 생각하면 된다.<br>\r\n우리의 상황에서는 정답(ex.1 0 0) 과 예측값인 (q1 q2 q3)간의 거리를 표현한다.</p>\n</blockquote>\n<p>정답을 (p1, p2 ,p3)라 하고, 예측을 (q1, q2 , q3)라 하면<br>\r\ncross-entropy는 다음과 같이 정의된다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218693654-dbd1afb2-c8f0-43ef-a4bf-93d68ed904ee.png\" alt=\"image\"><br>\r\n만약 소 라면, (1 0 0)이므로 -logq1이 나옴을 알 수 있다.<br>\r\n즉슨 위 식을 다음의 식으로 표현할 수 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218694465-df3e0b8b-5a09-4167-a149-5e273b1b9a62.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>Cross-entropy를 왜쓰냐?</p>\n</blockquote>\n<p>크로스 엔트로피는 엔트로피보다 항상 크거나 같다. 이를 식으로 표현하면 다음과 같다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218695641-aadbccba-4c88-4602-b1a7-332dcdf217e5.png\" alt=\"image\"><br>\r\n(우항이 엔트로피이다.)<br>\r\n이때 엔트로피를 풀어쓰면, <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218695980-ebce04f9-7570-47a4-86d1-091f146b3580.png\" alt=\"image\"><br>\r\n이고 정답데이터가 1 0 0 이면 0이 나온다. (즉, one-hot encoding에서는 0이 나온다.)<br>\r\n이말은 곧 lower bound가 0이 된다는 뜻이다. <br>\r\n그런데 좌항, 즉 크로스 엔트로피는 풀어쓰면 -logqj가 나온다. 즉 이 값을(loss를) 최대한 줄여서 0(엔트로피)을 만들거다.<br>\r\n그러면 -logqj가 0으로 가면 qj는 1이어야한다. 즉 qj를 1로 만드는, 자연스럽게 각 노드들이 각각의 클래스를 담당하도록. 설계된 것이다.<br></p>\n<br>\n<h2 id=\"5-마무리\" style=\"position:relative;\"><a href=\"#5-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"5 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. 마무리</h2>\n<p>갈수록 참 어렵다.<br>\r\n우선 오늘은 다중 분류를 살펴보았으나, 왜 이러한 activation, 이러한 structure, 이러한 loss를 썼는지에 대한 고찰을 더 해보는 시간을 가져야할 것 같다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/15/\">다음엔</a> universal approximation theorem을 다뤄보겠다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></p>\n</li>\n<li>\n<p><a href=\"#1-%EC%B6%9C%EB%A0%A5%EC%9D%80-%EB%AA%87%EA%B0%9C%EB%A1%9C-%ED%95%A0%EA%B9%8C\">1. 출력은 몇개로 할까?</a></p>\n<ul>\n<li><a href=\"#1-%EB%B6%88%EA%B3%B5%ED%8F%89%ED%95%9C-backpropagation\">(1) 불공평한 backpropagation</a></li>\n<li><a href=\"#2-%EB%B6%84%EB%A5%98-%EA%B0%80%EC%A7%93%EC%88%98%EA%B0%80-%EB%A7%8E%EC%95%84%EC%A7%80%EB%A9%B4\">(2) 분류 가짓수가 많아지면..?</a></li>\n<li><a href=\"#3-%EC%9D%B4%EC%83%81%ED%95%9C-%EC%82%AC%EC%A0%84-%EC%A0%95%EB%B3%B4\">(3) 이상한 사전 정보</a></li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-n%EA%B0%9C%EB%A1%9C-%ED%95%98%EC%9E%90\">2. n개로 하자!</a></p>\n</li>\n<li>\n<p><a href=\"#3-activation-functionsoftmax\">3. Activation function(softmax)</a></p>\n</li>\n<li>\n<p><a href=\"#4-loss-%EC%A0%95%EC%9D%98\">4. Loss 정의</a></p>\n</li>\n<li>\n<p><a href=\"#5-%EB%A7%88%EB%AC%B4%EB%A6%AC\">5. 마무리</a></p>\n</li>\n</ul>\n</div>","frontmatter":{"date":"February 14, 2023","title":"13. 다중분류는 어떻게 설계해야하는가?","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/14/"}},"prev":{"id":"75ef1949-7e48-5147-b4bf-f5b31b62d400","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/15/\">이전글</a>에서는 Universal Approximation Theorem을 공부했다.<br>\r\n이번글에서는 깊은 인공신경망의 문제점과 그 해결방안에 대해 다뤄보고자 한다.</p>\n<br>\r\n<br>\n<h2 id=\"1-깊은-인공신경망의-문제점\" style=\"position:relative;\"><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\" aria-label=\"1 깊은 인공신경망의 문제점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 깊은 인공신경망의 문제점</h2>\n<p>지금까지 우리는 인공신경망을 이용해서 딥러닝에 적용하는 방법에 대해 배웠다.<br>\r\n지난글에서는 Universal Approximation Theorem을 통해서 어떠한 연속함수든 근사해낼 수 있음도 알게 되었다.<br>\r\n그렇다면 인공신경망은 만능일까?<br>\r\n깊게 만들면 어떠한 문제든 다 해결할 수 있을까?<br>\r\n아쉽게도 그렇지만은 않다. 아무래도 그냥 DNN을 만드는것보다 중요한것은 ”<strong>잘</strong>“만드는 것이다.\r\n<br><br>\r\n생각없이 만들면 어떠한 문제에 봉착하는 것일까?<br>\r\n우리는 크게 2가지의 문제에 봉착한다.</p>\n<ul>\n<li><strong>Vanishing Gradient</strong></li>\n<li><strong>Overfitting</strong></li>\n<li>(bonus) loss landscape 의 non-convexity</li>\n</ul>\n<blockquote>\n<p>마지막 bonus는 초심자 입장에서 생각할 문제는 아니다.<br>\r\nloss의 landscape를 가능한 convex하게 만들면 GD가 잘 동작할 것이니, 이러한 점도 고려할 수 있다면 고려하자.<br>\r\n하지만 우리는 초심자이니까 앞의 두개에 대한 이야기를 하도록 하겠다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"2-vanishing-gradient\" style=\"position:relative;\"><a href=\"#2-vanishing-gradient\" aria-label=\"2 vanishing gradient permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Vanishing Gradient</h2>\n<p>vanishing gradient는 직역하면 ‘gradient가 사라지는것’ 정도로 해석할 수 있다.<br>\r\ngradient가 사라진다니? 직관적으로 설명하면, ”<strong>줄을 아무리 흔들어도 뒤까지 전달이 안된다</strong>“라고 비유할 수 있다. <br>\r\n혹시 backpropagation을 기억하는가? 깊은 곳의 weight는 chain rule이 연속적으로 적용되어 곱해지는 값들이 많다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219021253-cedaa2b6-0eac-46e0-8f44-d74db02adfb6.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>기억나지 않는 분들을 위해서 가져왔다.</p>\n</blockquote>\n<p>자 그런데 생각해보자. 우리가 지금까지 배운 activation 중 미분이 가능한 activation은 linear activation과 sigmoid이다.<br>\r\n그런데 linear activation은 신경망의 깊이 자체에 영향을 주지 못한다는 사실을 배웠다. 즉 깊은 인공신경망을 위해서는 sigmoid가 이용되어야한다는 것인데..<br>\r\nsigmoid는 미분의 최대값이 1/4이다. 즉 저 많은 곱셈연산중 activation 미분값이 4분의 1 이하라는 것이다.<br>\r\n그렇다면 4분의 1이 3번만 곱해져도 1/64로 0에 급속히 가까워진다.<br>\r\n즉 weight의 업데이트가 거의 일어나지 않게 되는 것이다.<br></p>\n<blockquote>\n<p>정리 : 깊은 곳의 weight일수록, Loss의 미분값이 매우 작아서(0에 가까워서) 업데이트가 거의 일어나지 않는 현상이 일어난다.</p>\n</blockquote>\n<p>이를 Vanishing Gradient라고 한다.<br>\r\nvanishing gradient가 일어나면, 학습이 잘 이뤄지지않고 오히려 underfitting이 일어나게 된다.(오지게 안맞는다ㅇㅇ)<br></p>\n<br>\r\n<br>\n<h2 id=\"3-sigmoid가-범인-해결책은\" style=\"position:relative;\"><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\" aria-label=\"3 sigmoid가 범인 해결책은 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Sigmoid가 범인! 해결책은?</h2>\n<p>vanishing gradient를 일으키는 범인은 누굴까? 역시 sigmoid의 작은 미분 값때문이 것이다.<br>\r\n따라서 이를 위해서 sigmoid를 포기하고 새로운 activation function을 찾아본다.<br>\r\n그리고 등장한 것이 바로 <strong>ReLU(Rectified Linear Unit)</strong> 이다.<br>\r\n그럼 ReLU는 어떻게 생겼을까? 매우 간단하게 생겼다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024070-aea3dc2c-2c4a-4f3c-8505-a83395fed72c.png\" alt=\"image\">\r\n<br>\r\n원점을 기준으로 음의 방향은 0, 양의 방향은 linear 한 모습을 갖고 있다.<br>\r\n<strong>그런데!</strong> 이런 모습을 띄고 있자니 문제가 있을 것 같다. 모든 input이 0보다 작으면 output이 0만 출력되는 극단적인 경우가 발생할 수 있다.<br>\r\n그래서 여러가지 변형된 버전의 ReLU가 등장하였는데 그중 대표적으로 Leaky ReLU와 Parametric ReLU가 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024822-fe799045-23d1-4408-be9f-3534869d0a79.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>leaky ReLU는 ‘다 0으로 죽이기 뭐하니까 살리자’라는 느낌<br>\r\nparametric ReLU는 ‘살리긴하는데 좀 근거 있게 살리자’라는 느낌</p>\n</blockquote>\n<p>parametric ReLU는 기울기를 학습을 통해서 결정한다. 즉 기울기 a도 미분에 참여한다.<br></p>\n<p><br><br></p>\n<h2 id=\"4-relu-그냥-다-살리면-안돼\" style=\"position:relative;\"><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\" aria-label=\"4 relu 그냥 다 살리면 안돼 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. ReLU! 그냥 다 살리면 안돼?</h2>\n<p>왜 애매하게 남겨두냐! 그냥 살리면 안되냐? 라고 생각하는 사람도 있을 수 있다. 그러나 그러한 경우 linear activation과 같아지므로 신경망이 깊어지는 효과를 누리지 못한다는 단점이 있다.<br>\r\n그래서 다시한번 정리하면, Linear activation은 이용될때 ‘마지막 출력 layer’에서만 쓰인다.(일반적으로 ㅇㅇ)<br></p>\n<br>\n<h2 id=\"5-relu의-자매품\" style=\"position:relative;\"><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\" aria-label=\"5 relu의 자매품 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. ReLU의 자매품</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219025975-74df55ff-b6eb-4d52-acbc-6ae49778a074.png\" alt=\"image\"><br>\r\nReLU 자매품이 다양하다. 관심이 있다면 하나씩 검색해서 살펴보자.</p>\n<p><br><br></p>\n<h2 id=\"6-마무리\" style=\"position:relative;\"><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"6 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219026480-1fca0e25-5b18-4b8c-98fd-ea8154aeee20.png\" alt=\"image\"><br></p>\n<p>이번글에서는 sigmoid의 한계로인해 새로운 activation ReLU에 대해서 살펴보았다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/17/\">다음글</a>에서는 vanishing gradient를 해결할 다른 방안에 대해서 살펴보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\">1. 깊은 인공신경망의 문제점</a></li>\n<li><a href=\"#2-vanishing-gradient\">2. Vanishing Gradient</a></li>\n<li><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\">3. Sigmoid가 범인! 해결책은?</a></li>\n<li><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\">4. ReLU! 그냥 다 살리면 안돼?</a></li>\n<li><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\">5. ReLU의 자매품</a></li>\n<li><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\">6. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"February 15, 2023","title":"15. 깊은 인공신경망의 문제1(feat. vanishing gradient)","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/16/"}},"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/15/","nextSlug":"/ai/14/","prevSlug":"/ai/16/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}