{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/16/",
    "result": {"data":{"cur":{"id":"75ef1949-7e48-5147-b4bf-f5b31b62d400","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/15/\">이전글</a>에서는 Universal Approximation Theorem을 공부했다.<br>\r\n이번글에서는 깊은 인공신경망의 문제점과 그 해결방안에 대해 다뤄보고자 한다.</p>\n<br>\r\n<br>\n<h2 id=\"1-깊은-인공신경망의-문제점\" style=\"position:relative;\"><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\" aria-label=\"1 깊은 인공신경망의 문제점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 깊은 인공신경망의 문제점</h2>\n<p>지금까지 우리는 인공신경망을 이용해서 딥러닝에 적용하는 방법에 대해 배웠다.<br>\r\n지난글에서는 Universal Approximation Theorem을 통해서 어떠한 연속함수든 근사해낼 수 있음도 알게 되었다.<br>\r\n그렇다면 인공신경망은 만능일까?<br>\r\n깊게 만들면 어떠한 문제든 다 해결할 수 있을까?<br>\r\n아쉽게도 그렇지만은 않다. 아무래도 그냥 DNN을 만드는것보다 중요한것은 ”<strong>잘</strong>“만드는 것이다.\r\n<br><br>\r\n생각없이 만들면 어떠한 문제에 봉착하는 것일까?<br>\r\n우리는 크게 2가지의 문제에 봉착한다.</p>\n<ul>\n<li><strong>Vanishing Gradient</strong></li>\n<li><strong>Overfitting</strong></li>\n<li>(bonus) loss landscape 의 non-convexity</li>\n</ul>\n<blockquote>\n<p>마지막 bonus는 초심자 입장에서 생각할 문제는 아니다.<br>\r\nloss의 landscape를 가능한 convex하게 만들면 GD가 잘 동작할 것이니, 이러한 점도 고려할 수 있다면 고려하자.<br>\r\n하지만 우리는 초심자이니까 앞의 두개에 대한 이야기를 하도록 하겠다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"2-vanishing-gradient\" style=\"position:relative;\"><a href=\"#2-vanishing-gradient\" aria-label=\"2 vanishing gradient permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Vanishing Gradient</h2>\n<p>vanishing gradient는 직역하면 ‘gradient가 사라지는것’ 정도로 해석할 수 있다.<br>\r\ngradient가 사라진다니? 직관적으로 설명하면, ”<strong>줄을 아무리 흔들어도 뒤까지 전달이 안된다</strong>“라고 비유할 수 있다. <br>\r\n혹시 backpropagation을 기억하는가? 깊은 곳의 weight는 chain rule이 연속적으로 적용되어 곱해지는 값들이 많다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219021253-cedaa2b6-0eac-46e0-8f44-d74db02adfb6.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>기억나지 않는 분들을 위해서 가져왔다.</p>\n</blockquote>\n<p>자 그런데 생각해보자. 우리가 지금까지 배운 activation 중 미분이 가능한 activation은 linear activation과 sigmoid이다.<br>\r\n그런데 linear activation은 신경망의 깊이 자체에 영향을 주지 못한다는 사실을 배웠다. 즉 깊은 인공신경망을 위해서는 sigmoid가 이용되어야한다는 것인데..<br>\r\nsigmoid는 미분의 최대값이 1/4이다. 즉 저 많은 곱셈연산중 activation 미분값이 4분의 1 이하라는 것이다.<br>\r\n그렇다면 4분의 1이 3번만 곱해져도 1/64로 0에 급속히 가까워진다.<br>\r\n즉 weight의 업데이트가 거의 일어나지 않게 되는 것이다.<br></p>\n<blockquote>\n<p>정리 : 깊은 곳의 weight일수록, Loss의 미분값이 매우 작아서(0에 가까워서) 업데이트가 거의 일어나지 않는 현상이 일어난다.</p>\n</blockquote>\n<p>이를 Vanishing Gradient라고 한다.<br>\r\nvanishing gradient가 일어나면, 학습이 잘 이뤄지지않고 오히려 underfitting이 일어나게 된다.(오지게 안맞는다ㅇㅇ)<br></p>\n<br>\r\n<br>\n<h2 id=\"3-sigmoid가-범인-해결책은\" style=\"position:relative;\"><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\" aria-label=\"3 sigmoid가 범인 해결책은 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Sigmoid가 범인! 해결책은?</h2>\n<p>vanishing gradient를 일으키는 범인은 누굴까? 역시 sigmoid의 작은 미분 값때문이 것이다.<br>\r\n따라서 이를 위해서 sigmoid를 포기하고 새로운 activation function을 찾아본다.<br>\r\n그리고 등장한 것이 바로 <strong>ReLU(Rectified Linear Unit)</strong> 이다.<br>\r\n그럼 ReLU는 어떻게 생겼을까? 매우 간단하게 생겼다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024070-aea3dc2c-2c4a-4f3c-8505-a83395fed72c.png\" alt=\"image\">\r\n<br>\r\n원점을 기준으로 음의 방향은 0, 양의 방향은 linear 한 모습을 갖고 있다.<br>\r\n<strong>그런데!</strong> 이런 모습을 띄고 있자니 문제가 있을 것 같다. 모든 input이 0보다 작으면 output이 0만 출력되는 극단적인 경우가 발생할 수 있다.<br>\r\n그래서 여러가지 변형된 버전의 ReLU가 등장하였는데 그중 대표적으로 Leaky ReLU와 Parametric ReLU가 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024822-fe799045-23d1-4408-be9f-3534869d0a79.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>leaky ReLU는 ‘다 0으로 죽이기 뭐하니까 살리자’라는 느낌<br>\r\nparametric ReLU는 ‘살리긴하는데 좀 근거 있게 살리자’라는 느낌</p>\n</blockquote>\n<p>parametric ReLU는 기울기를 학습을 통해서 결정한다. 즉 기울기 a도 미분에 참여한다.<br></p>\n<p><br><br></p>\n<h2 id=\"4-relu-그냥-다-살리면-안돼\" style=\"position:relative;\"><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\" aria-label=\"4 relu 그냥 다 살리면 안돼 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. ReLU! 그냥 다 살리면 안돼?</h2>\n<p>왜 애매하게 남겨두냐! 그냥 살리면 안되냐? 라고 생각하는 사람도 있을 수 있다. 그러나 그러한 경우 linear activation과 같아지므로 신경망이 깊어지는 효과를 누리지 못한다는 단점이 있다.<br>\r\n그래서 다시한번 정리하면, Linear activation은 이용될때 ‘마지막 출력 layer’에서만 쓰인다.(일반적으로 ㅇㅇ)<br></p>\n<br>\n<h2 id=\"5-relu의-자매품\" style=\"position:relative;\"><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\" aria-label=\"5 relu의 자매품 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. ReLU의 자매품</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219025975-74df55ff-b6eb-4d52-acbc-6ae49778a074.png\" alt=\"image\"><br>\r\nReLU 자매품이 다양하다. 관심이 있다면 하나씩 검색해서 살펴보자.</p>\n<p><br><br></p>\n<h2 id=\"6-마무리\" style=\"position:relative;\"><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"6 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219026480-1fca0e25-5b18-4b8c-98fd-ea8154aeee20.png\" alt=\"image\"><br></p>\n<p>이번글에서는 sigmoid의 한계로인해 새로운 activation ReLU에 대해서 살펴보았다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/17/\">다음글</a>에서는 vanishing gradient를 해결할 다른 방안에 대해서 살펴보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\">1. 깊은 인공신경망의 문제점</a></li>\n<li><a href=\"#2-vanishing-gradient\">2. Vanishing Gradient</a></li>\n<li><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\">3. Sigmoid가 범인! 해결책은?</a></li>\n<li><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\">4. ReLU! 그냥 다 살리면 안돼?</a></li>\n<li><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\">5. ReLU의 자매품</a></li>\n<li><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\">6. 마무리</a></li>\n</ul>\n</div>","excerpt":"0. 지난 이야기 이전글에서는 Universal Approximation Theorem을 공부했다.\r\n이번글에서는 깊은 인공신경망의 문제점과 그 해결방안에 대해 다뤄보고자 한다. 1. 깊은 인공신경망의 문제점 지금까지 우리는 인공신경망을 이용해서 딥러닝에 적용하는 방법에 대해 배웠다.\r\n지난글에서는 Universal Approximation Theorem을 통해서 어떠한 연속함수든 근사해낼 수 있음도 알게 되었다.\r\n그렇다면 인공신경망은 만능일까?\r\n깊게 만들면 어떠한 문제든 다 해결할 수 있을까?\r\n아쉽게도 그렇지만은 않다. 아무래도 그냥 DNN을 만드는것보다 중요한것은 ”잘“만드는 것이다.\r\n\r\n생각없이 만들면 어떠한 문제에 봉착하는 것일까?\r\n우리는 크게 2가지의 문제에 봉착한다. Vanishing Gradient Overfitting (bonus) loss landscape 의 non-convexity 마지막 bonus는 초심자 입장에서 생각할 문제는 아니다.\r\nloss의 …","frontmatter":{"date":"February 15, 2023","title":"15. 깊은 인공신경망의 문제1(feat. vanishing gradient)","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/16/"}},"next":{"id":"f5da3d83-113e-5df7-910c-14c6eb9e604b","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/218723923-26e429a5-1d3c-4c95-94aa-ac197c3bcd28.png\" alt=\"image\"><br>\r\n<a href=\"https://tannybrown.github.io/ai/14/\">이전글</a>에서 다중분류에 대해 알아보았다.<br>\r\n이번글에서는 인공신경망이 이렇게까지 쓰이는 이유가 뭘지 한번 알아보자.<br><br></p>\n<h2 id=\"1-인공신경망을-쓰는-과정\" style=\"position:relative;\"><a href=\"#1-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EC%93%B0%EB%8A%94-%EA%B3%BC%EC%A0%95\" aria-label=\"1 인공신경망을 쓰는 과정 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 인공신경망을 쓰는 과정</h2>\n<p>지금까지 공부한 내용을 summary 해보자.\r\n인공신경망을 이용하는 큰 4가지 step에 대해 공부해보았다.</p>\n<h4 id=\"step-1-입출력-정의\" style=\"position:relative;\"><a href=\"#step-1-%EC%9E%85%EC%B6%9C%EB%A0%A5-%EC%A0%95%EC%9D%98\" aria-label=\"step 1 입출력 정의 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 1. 입출력 정의</h4>\n<p>입출력을 정의해야했다. 입력을 독립변수, 출력을 종속변수라고 했으며, 출력이 여러개일수도 한개일수도 있었다.<br></p>\n<h4 id=\"step-2-모델-만들기\" style=\"position:relative;\"><a href=\"#step-2-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0\" aria-label=\"step 2 모델 만들기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 2. 모델 만들기</h4>\n<p>모델을 만들어야했다. 입력층과 출력층, 그리고 가장중요한 hidden layer의 구성.<br>\r\nhidden layer를 어떻게 디자인 하는가가 모델의 성능을 결정한다. 하지만 이에 대해서는 자세히 아직 다루지 않았고, CNN을 공부하며 공부해볼 것이다.<br></p>\n<h4 id=\"step-3-loss-정의\" style=\"position:relative;\"><a href=\"#step-3-loss-%EC%A0%95%EC%9D%98\" aria-label=\"step 3 loss 정의 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 3. loss 정의</h4>\n<p>모델까지 만들었으면 Loss를 정의해야했다. 주어진 입출력과 activation을 고려해서 Loss를 정의했고 지금까지 배운 loss는 MSE, log likelihood, cross-entropy가 있었다.<br></p>\n<h4 id=\"step-4-weight-bias-찾기\" style=\"position:relative;\"><a href=\"#step-4-weight-bias-%EC%B0%BE%EA%B8%B0\" aria-label=\"step 4 weight bias 찾기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>step 4. weight, bias 찾기</h4>\n<p>Loss 정의까지 마쳤다면, 이제 학습을 시켜야한다. 즉 최적의 weigth 와 bias를 찾는 과정이다. <br>\r\n이에 관해서는 gradient descent, SGD, mini-batch, adam 등을 공부했다. 새로운 알고리즘을 개발하는 것도 좋은 연구분야일 것이다.</p>\n<p><br><br></p>\n<h2 id=\"2-인공신경망을-굳이-써야하나\" style=\"position:relative;\"><a href=\"#2-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EA%B5%B3%EC%9D%B4-%EC%8D%A8%EC%95%BC%ED%95%98%EB%82%98\" aria-label=\"2 인공신경망을 굳이 써야하나 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 인공신경망을 굳이 써야하나?</h2>\n<p>결론적으로 모델이란건 <strong>함수</strong>였고, 인공신경망을 통해서 그 함수를 구현했다.<br>\r\n그런데 <strong>왜</strong> 굳이 인공신경망이라는 구조를 이용한걸까?<br>\r\n그 이유는 바로, 인공신경망을 이용하면 <strong>모든 연속 함수를 근사할 수 있기 때문</strong>이다.<br>\r\n이를 <strong>Universal Approximation Theorem</strong>이라고 한다.</p>\n<p><br><br></p>\n<h2 id=\"3-universal-approximation-theorem\" style=\"position:relative;\"><a href=\"#3-universal-approximation-theorem\" aria-label=\"3 universal approximation theorem permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Universal Approximation Theorem</h2>\n<p>Universal Approximation Theorem은 인공신경망의 근간이 되는 이론으로, 하나의 hiddle layer로 모든 연속 함수를 근사할 수 있다는 것인데, 예시를 살펴보며 이해해보자.<br>\r\n여기 키와 몸무게에 대한 데이터가 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218725303-6cf633d0-db44-4e96-94d3-22548a45a15b.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>등간격이 아닌점들은 양해를 바란다. 그냥 수치만 보자.</p>\n</blockquote>\n<p>키는 <strong>독립변수</strong>, 몸무게가 <strong>종속변수</strong>라고 해보자. 즉, 키가 주어졌을때 적절한 몸무게를 예측 하고 싶다.<br>\r\n이렇게 데이터가 주어졌다면 우린 이 데이터들을 만족하는 함수를 찾고 싶다.<br>\r\n이때 이용하는 전략은 각 데이터당 2개의 노드를 만들고, 그 두 노드를 이용해 loss가 0이 되도록 하는 함수를 만들어 보도록 하겠다.<br>\r\n우선은 가장 첫번째로, (키 : 150, 몸무게 : 70)데이터를 보자.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218773417-42232304-f199-4ade-b139-c664f0955675.png\" alt=\"image\"><br>\r\n두개의 노드를 이용해서 히든레이어를 구성했고 각각은 unit step function을 activation으로 갖고 있다.<br>\r\n이러한 경우에 출력은 이러한 폼이 된다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218774304-59cd9632-c24c-4e98-ae3f-e324694949e1.png\" alt=\"image\"><br>\r\n149~151사이의 데이터만 70의 출력을 갖는 개형이다.<br>\r\n이러한 짓을 모든 데이터에 대해 각각 한다면??<br>\r\n아마 주어진 데이터들에 모두 이 과정을 수행하면 다음과 같은 신경망이 그려질 것이다.<br></p>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/218775975-6e3668f2-3179-4024-96ec-0b192c8562c3.png\" alt=\"image\">\r\n<br>\r\n그리고 이 신경망의 출력을 그래프로 표현하면,<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/218776742-b500425c-b3d7-4ca1-8b4c-620110af19cb.png\" alt=\"image\"><br></p>\n<p>이와 같음을 알 수 있다.<br>\r\n더많은 데이터가 주어져도 마찬가지의 방법을 통한다면, Loss가 0인 fitting하는 연속함수를 만들 수 있음을 알 수 있다.<br></p>\n<blockquote>\n<p>이게 뭐야 그냥 어거지 아니냐?<br>\r\n그러한 생각을 할 수 있지만, ‘위의 예시는 이렇게 해서 만들 수 있다.‘를 설명하기 위한것이지. 이렇게 만들거다 라는 것은 아니다.<br>\r\n즉, 어떠한 데이터가 주어져도 그에 맞는 함수를 만들 수 있음을 보인것이다.</p>\n</blockquote>\n<p><br><br></p>\n<h2 id=\"4-마무리\" style=\"position:relative;\"><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"4 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/218778227-c093d8c5-fc11-4dd6-8480-5a0a35adb14b.png\" alt=\"image\"><br></p>\n<p>universal approximation theorem을 이용하면, 어떤 연속함수든 근사해낼 수 있었다.<br>\r\n이러한 정리를 바탕으로 인공신경망을 통해서 우리는 (overfitting이 일어나지 않게)잘 학습만 시키면 된다는 것을 알 수 있다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/16/\">다음글</a>에서는 인공신경망의 문제점들에 대한 이야기를 다뤄보겠다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li>\n<p><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></p>\n</li>\n<li>\n<p><a href=\"#1-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EC%93%B0%EB%8A%94-%EA%B3%BC%EC%A0%95\">1. 인공신경망을 쓰는 과정</a></p>\n<ul>\n<li>\n<ul>\n<li><a href=\"#step-1-%EC%9E%85%EC%B6%9C%EB%A0%A5-%EC%A0%95%EC%9D%98\">step 1. 입출력 정의</a></li>\n<li><a href=\"#step-2-%EB%AA%A8%EB%8D%B8-%EB%A7%8C%EB%93%A4%EA%B8%B0\">step 2. 모델 만들기</a></li>\n<li><a href=\"#step-3-loss-%EC%A0%95%EC%9D%98\">step 3. loss 정의</a></li>\n<li><a href=\"#step-4-weight-bias-%EC%B0%BE%EA%B8%B0\">step 4. weight, bias 찾기</a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p><a href=\"#2-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%84-%EA%B5%B3%EC%9D%B4-%EC%8D%A8%EC%95%BC%ED%95%98%EB%82%98\">2. 인공신경망을 굳이 써야하나?</a></p>\n</li>\n<li>\n<p><a href=\"#3-universal-approximation-theorem\">3. Universal Approximation Theorem</a></p>\n</li>\n<li>\n<p><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\">4. 마무리</a></p>\n</li>\n</ul>\n</div>","frontmatter":{"date":"February 14, 2023","title":"14. 인공신경망이 쓰이는 이유","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/15/"}},"prev":{"id":"c41816b3-faaa-5298-8c4a-b79d954d018d","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p>깊은 인공신경망의 문제점 중 하나인 vanishing gradient…<br>\r\n이를 해결하기 위한 방안인 <strong>ReLU</strong>에 대해 살펴보았다.<br>\r\n이번글에서는 또 다른 방안인 <strong>Batch Normalization</strong>에 대해 살펴보겠다.<br></p>\n<br>\r\n<br>\n<h2 id=\"1-batch-normalization\" style=\"position:relative;\"><a href=\"#1-batch-normalization\" aria-label=\"1 batch normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Batch Normalization</h2>\n<p>자 <a href=\"https://tannybrown.github.io/ai/16/\">이전시간</a>에 배운 ReLU를 이용하여 신경망을 구성한다고 해보자.<br>\r\nBatch Normalization의 batch는 mini-batch 알고리즘의 batch를 의미한다.<br>\r\nbatch normalization은 어떤 layer에 적용할지를 선택한다. (이는 하이퍼 파라미터이다 ㅇㅇ)<br>\r\n예시 그림에서는 첫번째 보이는 hidden layer에 batch normalization을 적용하려 한다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219054999-f17a5ee4-ded4-411d-848e-c128e7ebe60e.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>예시니까 간단한 구조로 그린것임</p>\n</blockquote>\n<p>자 이때 ReLU에 집중해보자.<br>\r\n만약 ReLU에 들어온 데이터가 모두 양수였다면, 모두 linear하게 나갈것이다.<br>\r\n그렇게 되면 이 액티베이션이 ReLU인지 linear activation인지 분간할 수가 없어진다. 즉 not linearity를 살려주지 못한다.<br>\r\n이를 방지하기 위해서(<strong>= not linearity를 살려주기 위해서</strong>) 우리가 정한 layer의 batch를 normalization해줄 것이다.<br>\r\n다음과 같이 말이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219058965-23502c51-059a-40c0-8363-5476b6cee86b.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>그러면 어떻게 Normalization을 하냐?\r\n걍 0근처로 옮기는 건가요?</p>\n</blockquote>\n<p>우선 위 그림과 같이 normalization이 되기 위해서는 <strong>평균</strong>과 <strong>표준편차</strong>가 필요하다.(정규화라는 작업 자체가 그렇다 ㅇㅇ)<br>\r\n그럼 평균과 표준편차를 우리가 정하는 것이냐?<br>\r\n<strong>아니다.</strong> batch normalization에서는 적절한 평균과 표준편차를 머신이 학습을 통해서 알아낸다.</p>\n<blockquote>\n<p>parametric ReLU가 기울기를 학습하는 방식과 같다.</p>\n</blockquote>\n<p>이러한 방식으로 vanishing gradient를 해결하는 방식을 batch normalization 이라고 한다.\r\n<br>\r\n<br></p>\n<h2 id=\"2-layer-normalization\" style=\"position:relative;\"><a href=\"#2-layer-normalization\" aria-label=\"2 layer normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Layer Normalization</h2>\n<p>Batch Normalization말고도 Layer Normalization이라는 방식 또한 존재한다.<br>\r\nLayer Normalization은 batch와는 관련이 없고, 각 레이어에서의 평균과 분산을 사용하여 각 입력 데이터의 정규화를 수행한다.<br>\r\n두 정규화에 대해 자세하게 다루진 않을 것이다. 우선은 이러한 정규화 기법들이 존재한다 정도를 기억하자.</p>\n<blockquote>\n<p>참고로, batch normalization은 배치 사이즈가 작으면 쓰기가 어렵다. 하지만 layer normalization은 배치사이즈와 무관하게 적용이 가능하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"3-그럼-해결된건가\" style=\"position:relative;\"><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\" aria-label=\"3 그럼 해결된건가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 그럼 해결된건가?</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219062495-831f07b4-2c5c-4a46-8760-64e755619dfa.png\" alt=\"image\"><br>\r\n안타깝게도 아니다. ReLU와 Batch Normalization을 적용하더라도 Loss의 개형이 복잡해져서 학습이 잘 일어나지 않을 수 있다.<br></p>\n<blockquote>\n<p>그러니 만병통치약은 없다. 상황에따라서 적절히 기법들을 적용하는 것이 중요하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"4-마무리\" style=\"position:relative;\"><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"4 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 마무리</h2>\n<p>이번글에서는 batch normalization에 대해 알아보았다.<br>\r\n다음글에서는 overfitting문제의 해결 기법에 대해 알아보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-batch-normalization\">1. Batch Normalization</a></li>\n<li><a href=\"#2-layer-normalization\">2. Layer Normalization</a></li>\n<li><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\">3. 그럼 해결된건가?</a></li>\n<li><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\">4. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"February 16, 2023","title":"15. 깊은 인공신경망의 문제 2편","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/17/"}},"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/16/","nextSlug":"/ai/15/","prevSlug":"/ai/17/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}