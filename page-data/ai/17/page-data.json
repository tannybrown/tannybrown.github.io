{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/17/",
    "result": {"data":{"cur":{"id":"c41816b3-faaa-5298-8c4a-b79d954d018d","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p>깊은 인공신경망의 문제점 중 하나인 vanishing gradient…<br>\r\n이를 해결하기 위한 방안인 <strong>ReLU</strong>에 대해 살펴보았다.<br>\r\n이번글에서는 또 다른 방안인 <strong>Batch Normalization</strong>에 대해 살펴보겠다.<br></p>\n<br>\r\n<br>\n<h2 id=\"1-batch-normalization\" style=\"position:relative;\"><a href=\"#1-batch-normalization\" aria-label=\"1 batch normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Batch Normalization</h2>\n<p>자 <a href=\"https://tannybrown.github.io/ai/16/\">이전시간</a>에 배운 ReLU를 이용하여 신경망을 구성한다고 해보자.<br>\r\nBatch Normalization의 batch는 mini-batch 알고리즘의 batch를 의미한다.<br>\r\nbatch normalization은 어떤 layer에 적용할지를 선택한다. (이는 하이퍼 파라미터이다 ㅇㅇ)<br>\r\n예시 그림에서는 첫번째 보이는 hidden layer에 batch normalization을 적용하려 한다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219054999-f17a5ee4-ded4-411d-848e-c128e7ebe60e.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>예시니까 간단한 구조로 그린것임</p>\n</blockquote>\n<p>자 이때 ReLU에 집중해보자.<br>\r\n만약 ReLU에 들어온 데이터가 모두 양수였다면, 모두 linear하게 나갈것이다.<br>\r\n그렇게 되면 이 액티베이션이 ReLU인지 linear activation인지 분간할 수가 없어진다. 즉 not linearity를 살려주지 못한다.<br>\r\n이를 방지하기 위해서(<strong>= not linearity를 살려주기 위해서</strong>) 우리가 정한 layer의 batch를 normalization해줄 것이다.<br>\r\n다음과 같이 말이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219058965-23502c51-059a-40c0-8363-5476b6cee86b.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>그러면 어떻게 Normalization을 하냐?\r\n걍 0근처로 옮기는 건가요?</p>\n</blockquote>\n<p>우선 위 그림과 같이 normalization이 되기 위해서는 <strong>평균</strong>과 <strong>표준편차</strong>가 필요하다.(정규화라는 작업 자체가 그렇다 ㅇㅇ)<br>\r\n그럼 평균과 표준편차를 우리가 정하는 것이냐?<br>\r\n<strong>아니다.</strong> batch normalization에서는 적절한 평균과 표준편차를 머신이 학습을 통해서 알아낸다.</p>\n<blockquote>\n<p>parametric ReLU가 기울기를 학습하는 방식과 같다.</p>\n</blockquote>\n<p>이러한 방식으로 vanishing gradient를 해결하는 방식을 batch normalization 이라고 한다.\r\n<br>\r\n<br></p>\n<h2 id=\"2-layer-normalization\" style=\"position:relative;\"><a href=\"#2-layer-normalization\" aria-label=\"2 layer normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Layer Normalization</h2>\n<p>Batch Normalization말고도 Layer Normalization이라는 방식 또한 존재한다.<br>\r\nLayer Normalization은 batch와는 관련이 없고, 각 레이어에서의 평균과 분산을 사용하여 각 입력 데이터의 정규화를 수행한다.<br>\r\n두 정규화에 대해 자세하게 다루진 않을 것이다. 우선은 이러한 정규화 기법들이 존재한다 정도를 기억하자.</p>\n<blockquote>\n<p>참고로, batch normalization은 배치 사이즈가 작으면 쓰기가 어렵다. 하지만 layer normalization은 배치사이즈와 무관하게 적용이 가능하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"3-그럼-해결된건가\" style=\"position:relative;\"><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\" aria-label=\"3 그럼 해결된건가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 그럼 해결된건가?</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219062495-831f07b4-2c5c-4a46-8760-64e755619dfa.png\" alt=\"image\"><br>\r\n안타깝게도 아니다. ReLU와 Batch Normalization을 적용하더라도 Loss의 개형이 복잡해져서 학습이 잘 일어나지 않을 수 있다.<br></p>\n<blockquote>\n<p>그러니 만병통치약은 없다. 상황에따라서 적절히 기법들을 적용하는 것이 중요하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"4-마무리\" style=\"position:relative;\"><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"4 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 마무리</h2>\n<p>이번글에서는 batch normalization에 대해 알아보았다.<br>\r\n다음글에서는 overfitting문제의 해결 기법에 대해 알아보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-batch-normalization\">1. Batch Normalization</a></li>\n<li><a href=\"#2-layer-normalization\">2. Layer Normalization</a></li>\n<li><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\">3. 그럼 해결된건가?</a></li>\n<li><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\">4. 마무리</a></li>\n</ul>\n</div>","excerpt":"0. 지난 이야기 깊은 인공신경망의 문제점 중 하나인 vanishing gradient…\r\n이를 해결하기 위한 방안인 ReLU에 대해 살펴보았다.\r\n이번글에서는 또 다른 방안인 Batch Normalization에 대해 살펴보겠다. 1. Batch Normalization 자 이전시간에 배운 ReLU를 이용하여 신경망을 구성한다고 해보자.\r\nBatch Normalization의 batch는 mini-batch 알고리즘의 batch를 의미한다.\r\nbatch normalization은 어떤 layer에 적용할지를 선택한다. (이는 하이퍼 파라미터이다 ㅇㅇ)\r\n예시 그림에서는 첫번째 보이는 hidden layer에 batch normalization을 적용하려 한다.\r\nimage 예시니까 간단한 구조로 그린것임 자 이때 ReLU에 집중해보자.\r\n만약 ReLU에 들어온 데이터가 모두 양수였다면, 모두 linear하게 나갈것이다.\r\n그렇게 되면 이 액티베이션이 ReLU인지 linear ac…","frontmatter":{"date":"February 16, 2023","title":"16. 깊은 인공신경망의 문제 2편","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/17/"}},"next":{"id":"75ef1949-7e48-5147-b4bf-f5b31b62d400","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/15/\">이전글</a>에서는 Universal Approximation Theorem을 공부했다.<br>\r\n이번글에서는 깊은 인공신경망의 문제점과 그 해결방안에 대해 다뤄보고자 한다.</p>\n<br>\r\n<br>\n<h2 id=\"1-깊은-인공신경망의-문제점\" style=\"position:relative;\"><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\" aria-label=\"1 깊은 인공신경망의 문제점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 깊은 인공신경망의 문제점</h2>\n<p>지금까지 우리는 인공신경망을 이용해서 딥러닝에 적용하는 방법에 대해 배웠다.<br>\r\n지난글에서는 Universal Approximation Theorem을 통해서 어떠한 연속함수든 근사해낼 수 있음도 알게 되었다.<br>\r\n그렇다면 인공신경망은 만능일까?<br>\r\n깊게 만들면 어떠한 문제든 다 해결할 수 있을까?<br>\r\n아쉽게도 그렇지만은 않다. 아무래도 그냥 DNN을 만드는것보다 중요한것은 ”<strong>잘</strong>“만드는 것이다.\r\n<br><br>\r\n생각없이 만들면 어떠한 문제에 봉착하는 것일까?<br>\r\n우리는 크게 2가지의 문제에 봉착한다.</p>\n<ul>\n<li><strong>Vanishing Gradient</strong></li>\n<li><strong>Overfitting</strong></li>\n<li>(bonus) loss landscape 의 non-convexity</li>\n</ul>\n<blockquote>\n<p>마지막 bonus는 초심자 입장에서 생각할 문제는 아니다.<br>\r\nloss의 landscape를 가능한 convex하게 만들면 GD가 잘 동작할 것이니, 이러한 점도 고려할 수 있다면 고려하자.<br>\r\n하지만 우리는 초심자이니까 앞의 두개에 대한 이야기를 하도록 하겠다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"2-vanishing-gradient\" style=\"position:relative;\"><a href=\"#2-vanishing-gradient\" aria-label=\"2 vanishing gradient permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Vanishing Gradient</h2>\n<p>vanishing gradient는 직역하면 ‘gradient가 사라지는것’ 정도로 해석할 수 있다.<br>\r\ngradient가 사라진다니? 직관적으로 설명하면, ”<strong>줄을 아무리 흔들어도 뒤까지 전달이 안된다</strong>“라고 비유할 수 있다. <br>\r\n혹시 backpropagation을 기억하는가? 깊은 곳의 weight는 chain rule이 연속적으로 적용되어 곱해지는 값들이 많다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219021253-cedaa2b6-0eac-46e0-8f44-d74db02adfb6.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>기억나지 않는 분들을 위해서 가져왔다.</p>\n</blockquote>\n<p>자 그런데 생각해보자. 우리가 지금까지 배운 activation 중 미분이 가능한 activation은 linear activation과 sigmoid이다.<br>\r\n그런데 linear activation은 신경망의 깊이 자체에 영향을 주지 못한다는 사실을 배웠다. 즉 깊은 인공신경망을 위해서는 sigmoid가 이용되어야한다는 것인데..<br>\r\nsigmoid는 미분의 최대값이 1/4이다. 즉 저 많은 곱셈연산중 activation 미분값이 4분의 1 이하라는 것이다.<br>\r\n그렇다면 4분의 1이 3번만 곱해져도 1/64로 0에 급속히 가까워진다.<br>\r\n즉 weight의 업데이트가 거의 일어나지 않게 되는 것이다.<br></p>\n<blockquote>\n<p>정리 : 깊은 곳의 weight일수록, Loss의 미분값이 매우 작아서(0에 가까워서) 업데이트가 거의 일어나지 않는 현상이 일어난다.</p>\n</blockquote>\n<p>이를 Vanishing Gradient라고 한다.<br>\r\nvanishing gradient가 일어나면, 학습이 잘 이뤄지지않고 오히려 underfitting이 일어나게 된다.(오지게 안맞는다ㅇㅇ)<br></p>\n<br>\r\n<br>\n<h2 id=\"3-sigmoid가-범인-해결책은\" style=\"position:relative;\"><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\" aria-label=\"3 sigmoid가 범인 해결책은 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Sigmoid가 범인! 해결책은?</h2>\n<p>vanishing gradient를 일으키는 범인은 누굴까? 역시 sigmoid의 작은 미분 값때문이 것이다.<br>\r\n따라서 이를 위해서 sigmoid를 포기하고 새로운 activation function을 찾아본다.<br>\r\n그리고 등장한 것이 바로 <strong>ReLU(Rectified Linear Unit)</strong> 이다.<br>\r\n그럼 ReLU는 어떻게 생겼을까? 매우 간단하게 생겼다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024070-aea3dc2c-2c4a-4f3c-8505-a83395fed72c.png\" alt=\"image\">\r\n<br>\r\n원점을 기준으로 음의 방향은 0, 양의 방향은 linear 한 모습을 갖고 있다.<br>\r\n<strong>그런데!</strong> 이런 모습을 띄고 있자니 문제가 있을 것 같다. 모든 input이 0보다 작으면 output이 0만 출력되는 극단적인 경우가 발생할 수 있다.<br>\r\n그래서 여러가지 변형된 버전의 ReLU가 등장하였는데 그중 대표적으로 Leaky ReLU와 Parametric ReLU가 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024822-fe799045-23d1-4408-be9f-3534869d0a79.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>leaky ReLU는 ‘다 0으로 죽이기 뭐하니까 살리자’라는 느낌<br>\r\nparametric ReLU는 ‘살리긴하는데 좀 근거 있게 살리자’라는 느낌</p>\n</blockquote>\n<p>parametric ReLU는 기울기를 학습을 통해서 결정한다. 즉 기울기 a도 미분에 참여한다.<br></p>\n<p><br><br></p>\n<h2 id=\"4-relu-그냥-다-살리면-안돼\" style=\"position:relative;\"><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\" aria-label=\"4 relu 그냥 다 살리면 안돼 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. ReLU! 그냥 다 살리면 안돼?</h2>\n<p>왜 애매하게 남겨두냐! 그냥 살리면 안되냐? 라고 생각하는 사람도 있을 수 있다. 그러나 그러한 경우 linear activation과 같아지므로 신경망이 깊어지는 효과를 누리지 못한다는 단점이 있다.<br>\r\n그래서 다시한번 정리하면, Linear activation은 이용될때 ‘마지막 출력 layer’에서만 쓰인다.(일반적으로 ㅇㅇ)<br></p>\n<br>\n<h2 id=\"5-relu의-자매품\" style=\"position:relative;\"><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\" aria-label=\"5 relu의 자매품 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. ReLU의 자매품</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219025975-74df55ff-b6eb-4d52-acbc-6ae49778a074.png\" alt=\"image\"><br>\r\nReLU 자매품이 다양하다. 관심이 있다면 하나씩 검색해서 살펴보자.</p>\n<p><br><br></p>\n<h2 id=\"6-마무리\" style=\"position:relative;\"><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"6 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219026480-1fca0e25-5b18-4b8c-98fd-ea8154aeee20.png\" alt=\"image\"><br></p>\n<p>이번글에서는 sigmoid의 한계로인해 새로운 activation ReLU에 대해서 살펴보았다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/17/\">다음글</a>에서는 vanishing gradient를 해결할 다른 방안에 대해서 살펴보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\">1. 깊은 인공신경망의 문제점</a></li>\n<li><a href=\"#2-vanishing-gradient\">2. Vanishing Gradient</a></li>\n<li><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\">3. Sigmoid가 범인! 해결책은?</a></li>\n<li><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\">4. ReLU! 그냥 다 살리면 안돼?</a></li>\n<li><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\">5. ReLU의 자매품</a></li>\n<li><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\">6. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"February 15, 2023","title":"15. 깊은 인공신경망의 문제 1편(feat. vanishing gradient)","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/16/"}},"prev":{"id":"168da2c2-0f3e-5f80-8b28-7efc66e0d9be","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/17/\">지난글</a>에서 vanishing gradient의 해결법에 대해 살펴보았다.<br>\r\n이번글에서는 나머지 문제인 overfitting을 해결하는 방법에 대해 이야기 해보자.<br>\r\n<br></p>\n<h2 id=\"1-overfitting오버피팅-과적합이란\" style=\"position:relative;\"><a href=\"#1-overfitting%EC%98%A4%EB%B2%84%ED%94%BC%ED%8C%85-%EA%B3%BC%EC%A0%81%ED%95%A9%EC%9D%B4%EB%9E%80\" aria-label=\"1 overfitting오버피팅 과적합이란 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Overfitting(오버피팅, 과적합)이란?</h2>\n<p>한마디로 표현하자면 ‘training data에 대해서는 성능이 좋은데 test data에 대해서는 성능이 나쁘다.’ 라고 할 수 있다.<br>\r\n그림으로 한번 알아보자.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/220355918-0d474e6b-9e17-434e-91a8-65058508b8e7.png\" alt=\"image\"><br>\r\n검정색 곡선이 ‘오버피팅된 모델’이고, 오버피팅 되지 않은 직선 모델이 있다.<br>\r\n이때 찍혀있는 점들은 training data라고 할때, training data에 대해서 검정색 곡선은 loss가 0임을 알 수 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/220356770-7c49eb3d-0ad3-4bc4-b5a1-4ba11bf3fe9a.png\" alt=\"image\">\r\n<br>\r\n그러나, 위 그림처럼 test data가 주어지면, 검정색 곡선보다 직선모델이 예측을 더 잘하고 있음을 알 수 있다.<br>\r\n우리가 모델을 학습시키는 이유는 새로운 데이터들에 대해서 예측을 잘 하기 위함이다. <br>\r\n따라서 오버피팅이 일어나지 않도록 모델을 설계하고 학습시키는 것은 중요한 사항이다.<br></p>\n<br>\n<h2 id=\"2-오버피팅의-원인\" style=\"position:relative;\"><a href=\"#2-%EC%98%A4%EB%B2%84%ED%94%BC%ED%8C%85%EC%9D%98-%EC%9B%90%EC%9D%B8\" aria-label=\"2 오버피팅의 원인 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. 오버피팅의 원인</h2>\n<p>오버피팅은 그럼 왜 일어나는 걸까?<br></p>\n<ol>\n<li>부족한 데이터</li>\n<li>데이터셋의 노이즈나 이상치</li>\n<li>모델의 복잡도</li>\n<li>학습 데이터셋과 테스트 데이터셋의 차이 <br></li>\n</ol>\n<p>등의 다양한 이유를 꼽을 수 있다.\r\n<br><br></p>\n<h2 id=\"3-해법1--모델-경량화\" style=\"position:relative;\"><a href=\"#3-%ED%95%B4%EB%B2%951--%EB%AA%A8%EB%8D%B8-%EA%B2%BD%EB%9F%89%ED%99%94\" aria-label=\"3 해법1  모델 경량화 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 해법1 : 모델 경량화</h2>\n<p>위 예시에서 살펴본 것처럼 모델이 꾸불꾸불 복잡하면 test데이터에는 딱 맞게 피팅될지 몰라도 test 데이터에 대해서는 낮은 성능을 보여준다.<br>\r\n따라서 모델이 너무 복잡하지 않게 경량화 하는 것이 첫번째 해결방안이 될 수 있다.\r\n<br><br></p>\n<h2 id=\"4-해법2--data-augmentation\" style=\"position:relative;\"><a href=\"#4-%ED%95%B4%EB%B2%952--data-augmentation\" aria-label=\"4 해법2  data augmentation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 해법2 : Data augmentation</h2>\n<p>모델을 적절히 경량화 했음에도 불구하고 오버피팅이 일어난다면, 데이터가 부족하지 않은지 체크를 해보자.<br>\r\n데이터를 더 많이 수집할 수 있다면 정말 좋겠지만, 대부분의 경우 추가적인 데이터를 공급받기가 어려울 것이다.<br>\r\n이런 경우에 사용할 수 있는 방법이 Data augmentation인데, 이는 <strong>기존에 갖고 있는 데이터를 변행해서 새로운 데이터를 생성하는 방법</strong>이다.<br>\r\n예를 들어 이미지 데이터의 경우, 이미지를 회전시키거나 뒤집기, 크기 조절, 색조 변화 등을 가하는 것이다. 이후 데이터셋이 이 데이터들을 추가하고 학습시킴으로써 모델의 성능을 향상시킨다.<br></p>\n<blockquote>\n<p>하지만 이미지를 왜곡시키거나 완전히 새로운 객체를 생성하는 것은 적용할 수 없다는 한계가 있다.<br>\r\n또, 일부 데이터 셋에 대해서는 적용할 수 없기도 하다.</p>\n</blockquote>\n<p><br><br></p>\n<h2 id=\"5-해법3--drop-out\" style=\"position:relative;\"><a href=\"#5-%ED%95%B4%EB%B2%953--drop-out\" aria-label=\"5 해법3  drop out permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. 해법3 : Drop-out</h2>\n<p>드롭아웃은 ‘학습 과정에서 신경망의 일부 노드를 무작위로 제거하는 방법’이다.<br>\r\n모델이 너무 복잡하니까, 있는 노드를 가려가면서 학습시킨다면 어떨까? 하는 아이디어 라고 볼 수 있다.</p>\n<blockquote>\n<p>영구적으로 제거해버리는 건가요?</p>\n</blockquote>\n<p>영구적인 제거는 아니다. 내가 drop out을 적용하고자 하는 노드를 선택하고, 그 노드가 사라질 확률 p를 정해준다.<br>\r\nDrop out을 만든 분께서는 여러개의 thinned network를 이용해서 학습시키는 것이라고 표현하기도 했다.</p>\n<blockquote>\n<p>일반적으로 입력층 or hidden layer에 적용한다.</p>\n</blockquote>\n<p><strong>하나의 데이터가 들어올때마다</strong> p에 따라 노드가 존재하거나 사라진다.<br></p>\n<blockquote>\n<p>배치마다, 에포크마다 가 아니다. ‘데이터마다’이다.</p>\n</blockquote>\n<p>이렇게 트레이닝을 시킨후, 테스트를 진행할때는 유의할 점이 있다.<br>\r\n바로, test때는 나가는 weight에 p를 곱해주는 것이다.\r\n<img src=\"https://user-images.githubusercontent.com/121401159/220367101-5d52b15b-63f3-4c6d-813c-4d62dd251eda.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>이를 통해서 여러 thinned network를 평균낸 효과를 얻고자 함이다.<br>\r\n아물론, 테스트 때는 노드가 drop 되지 않는다. 모두 사용한다.</p>\n</blockquote>\n<p>추가로 Drop-connect라는 방법도 있는데, 이는 노드가 아닌 connection을 랜덤하게 끊어버리고 학습시키는 방법이다.<br>\r\n조금더 일반화된 Drop-out 이라고 볼 수 있다.</p>\n<p><br><br></p>\n<h2 id=\"6-해법4--lp-regularization\" style=\"position:relative;\"><a href=\"#6-%ED%95%B4%EB%B2%954--lp-regularization\" aria-label=\"6 해법4  lp regularization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 해법4 : Lp-Regularization</h2>\n<p>Lp-Regularization은 모델의 가중치를 제한하여 과적합을 방지하는 기술이다.<br>\r\nLp-regularization은 가중치(w)의 크기를 제한하기 위해, 정규화(regularization) 항을 손실 함수에 추가한다. <br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/220370395-822cea00-5e01-46b7-8a0d-d669005e0d2d.png\" alt=\"image\"><br>\r\n이 때, p의 값에 따라 L1-regularization과 L2-regularization으로 구분된다.<br>\r\n우선, ||w|| 표현, norm 표현이 어색할테니 이를 먼저 살펴보자.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/220378831-ec3e3b1a-835c-402b-8907-7a26bf69e6f2.png\" alt=\"image\"><br>\r\n이라고 해보자.<br></p>\n<p>먼저 <strong>p가 2 인경우(L2-Regularazation)</strong><br></p>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/220379319-12818698-acc1-48bc-b169-6832c8a27747.png\" alt=\"image\"><br>\r\n과 같이 구할 수 있다.\r\n<br>\r\n<strong>p가 1인 경우(L1-Regularazation)</strong><br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/220379743-1896fb4f-3837-4efa-a32d-c73b9a838256.png\" alt=\"image\"><br>\r\n로 구할 수 있다.\r\n<br>\r\n그럼 람다의 역할은 뭘까? <br>람다는 웨이트를 얼마나 고려할지를 결정한다. 람다가 크다면 웨이트를 많이 고려하고 작다면 적게 고려한다.<br></p>\n<blockquote>\n<p>L1과 L2의 semantic한 차이는 무엇인가요?</p>\n</blockquote>\n<p>L2는 L2-norm이 이차함수 꼴이므로 기울기를 실펴본다면 w값의 크기가 작으면 작게 변하고 크면 크게 변함을 알 수 있다.<br>\r\n마찬가지로 L1은 L1-norm이 일차함수 꼴이므로, w값의 크기가 어떻든 loss를 일정하게 변하게 함을 알 수 있다.</p>\n<p><br><br></p>\n<h2 id=\"7-해법5--검증-데이터-이용\" style=\"position:relative;\"><a href=\"#7-%ED%95%B4%EB%B2%955--%EA%B2%80%EC%A6%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%B4%EC%9A%A9\" aria-label=\"7 해법5  검증 데이터 이용 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>7. 해법5 : 검증 데이터 이용</h2>\n<p>validation data를 이용하는 것에 대해서는 앞에서 다루었기 때문에 자세한 설명은 생략하도록 하겠다.<br>\r\n내용이 궁금하다면 <a href=\"https://tannybrown.github.io/ai/9/\">이곳</a>을 참고하길 바란다.<br><br></p>\n<h2 id=\"8-마무리\" style=\"position:relative;\"><a href=\"#8-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"8 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>8. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/220384458-a3409a25-0451-460a-85f9-594e54379036.png\" alt=\"image\"><br></p>\n<p>이번글에서는 오버피팅이 무엇이고, 오버피팅을 방지하는 방법들에 대해 살펴보았다.<br>\r\n다음글에서는 CNN에 대해 다뤄보겠다.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-overfitting%EC%98%A4%EB%B2%84%ED%94%BC%ED%8C%85-%EA%B3%BC%EC%A0%81%ED%95%A9%EC%9D%B4%EB%9E%80\">1. Overfitting(오버피팅, 과적합)이란?</a></li>\n<li><a href=\"#2-%EC%98%A4%EB%B2%84%ED%94%BC%ED%8C%85%EC%9D%98-%EC%9B%90%EC%9D%B8\">2. 오버피팅의 원인</a></li>\n<li><a href=\"#3-%ED%95%B4%EB%B2%951--%EB%AA%A8%EB%8D%B8-%EA%B2%BD%EB%9F%89%ED%99%94\">3. 해법1 : 모델 경량화</a></li>\n<li><a href=\"#4-%ED%95%B4%EB%B2%952--data-augmentation\">4. 해법2 : Data augmentation</a></li>\n<li><a href=\"#5-%ED%95%B4%EB%B2%953--drop-out\">5. 해법3 : Drop-out</a></li>\n<li><a href=\"#6-%ED%95%B4%EB%B2%954--lp-regularization\">6. 해법4 : Lp-Regularization</a></li>\n<li><a href=\"#7-%ED%95%B4%EB%B2%955--%EA%B2%80%EC%A6%9D-%EB%8D%B0%EC%9D%B4%ED%84%B0-%EC%9D%B4%EC%9A%A9\">7. 해법5 : 검증 데이터 이용</a></li>\n<li><a href=\"#8-%EB%A7%88%EB%AC%B4%EB%A6%AC\">8. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"February 21, 2023","title":"17. 깊은 인공신경망의 문제 3편(feat. overfitting)","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/18/"}},"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/17/","nextSlug":"/ai/16/","prevSlug":"/ai/18/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}