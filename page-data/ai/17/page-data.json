{
    "componentChunkName": "component---src-templates-blog-template-js",
    "path": "/ai/17/",
    "result": {"data":{"cur":{"id":"c41816b3-faaa-5298-8c4a-b79d954d018d","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p>깊은 인공신경망의 문제점 중 하나인 vanishing gradient…<br>\r\n이를 해결하기 위한 방안인 <strong>ReLU</strong>에 대해 살펴보았다.<br>\r\n이번글에서는 또 다른 방안인 <strong>Batch Normalization</strong>에 대해 살펴보겠다.<br></p>\n<br>\r\n<br>\n<h2 id=\"1-batch-normalization\" style=\"position:relative;\"><a href=\"#1-batch-normalization\" aria-label=\"1 batch normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. Batch Normalization</h2>\n<p>자 <a href=\"https://tannybrown.github.io/ai/16/\">이전시간</a>에 배운 ReLU를 이용하여 신경망을 구성한다고 해보자.<br>\r\nBatch Normalization의 batch는 mini-batch 알고리즘의 batch를 의미한다.<br>\r\nbatch normalization은 어떤 layer에 적용할지를 선택한다. (이는 하이퍼 파라미터이다 ㅇㅇ)<br>\r\n예시 그림에서는 첫번째 보이는 hidden layer에 batch normalization을 적용하려 한다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219054999-f17a5ee4-ded4-411d-848e-c128e7ebe60e.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>예시니까 간단한 구조로 그린것임</p>\n</blockquote>\n<p>자 이때 ReLU에 집중해보자.<br>\r\n만약 ReLU에 들어온 데이터가 모두 양수였다면, 모두 linear하게 나갈것이다.<br>\r\n그렇게 되면 이 액티베이션이 ReLU인지 linear activation인지 분간할 수가 없어진다. 즉 not linearity를 살려주지 못한다.<br>\r\n이를 방지하기 위해서(<strong>= not linearity를 살려주기 위해서</strong>) 우리가 정한 layer의 batch를 normalization해줄 것이다.<br>\r\n다음과 같이 말이다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219058965-23502c51-059a-40c0-8363-5476b6cee86b.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>그러면 어떻게 Normalization을 하냐?\r\n걍 0근처로 옮기는 건가요?</p>\n</blockquote>\n<p>우선 위 그림과 같이 normalization이 되기 위해서는 <strong>평균</strong>과 <strong>표준편차</strong>가 필요하다.(정규화라는 작업 자체가 그렇다 ㅇㅇ)<br>\r\n그럼 평균과 표준편차를 우리가 정하는 것이냐?<br>\r\n<strong>아니다.</strong> batch normalization에서는 적절한 평균과 표준편차를 머신이 학습을 통해서 알아낸다.</p>\n<blockquote>\n<p>parametric ReLU가 기울기를 학습하는 방식과 같다.</p>\n</blockquote>\n<p>이러한 방식으로 vanishing gradient를 해결하는 방식을 batch normalization 이라고 한다.\r\n<br>\r\n<br></p>\n<h2 id=\"2-layer-normalization\" style=\"position:relative;\"><a href=\"#2-layer-normalization\" aria-label=\"2 layer normalization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Layer Normalization</h2>\n<p>Batch Normalization말고도 Layer Normalization이라는 방식 또한 존재한다.<br>\r\nLayer Normalization은 batch와는 관련이 없고, 각 레이어에서의 평균과 분산을 사용하여 각 입력 데이터의 정규화를 수행한다.<br>\r\n두 정규화에 대해 자세하게 다루진 않을 것이다. 우선은 이러한 정규화 기법들이 존재한다 정도를 기억하자.</p>\n<blockquote>\n<p>참고로, batch normalization은 배치 사이즈가 작으면 쓰기가 어렵다. 하지만 layer normalization은 배치사이즈와 무관하게 적용이 가능하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"3-그럼-해결된건가\" style=\"position:relative;\"><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\" aria-label=\"3 그럼 해결된건가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. 그럼 해결된건가?</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219062495-831f07b4-2c5c-4a46-8760-64e755619dfa.png\" alt=\"image\"><br>\r\n안타깝게도 아니다. ReLU와 Batch Normalization을 적용하더라도 Loss의 개형이 복잡해져서 학습이 잘 일어나지 않을 수 있다.<br></p>\n<blockquote>\n<p>그러니 만병통치약은 없다. 상황에따라서 적절히 기법들을 적용하는 것이 중요하다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"4-마무리\" style=\"position:relative;\"><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"4 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. 마무리</h2>\n<p>이번글에서는 batch normalization에 대해 알아보았다.<br>\r\n다음글에서는 overfitting문제의 해결 기법에 대해 알아보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-batch-normalization\">1. Batch Normalization</a></li>\n<li><a href=\"#2-layer-normalization\">2. Layer Normalization</a></li>\n<li><a href=\"#3-%EA%B7%B8%EB%9F%BC-%ED%95%B4%EA%B2%B0%EB%90%9C%EA%B1%B4%EA%B0%80\">3. 그럼 해결된건가?</a></li>\n<li><a href=\"#4-%EB%A7%88%EB%AC%B4%EB%A6%AC\">4. 마무리</a></li>\n</ul>\n</div>","excerpt":"0. 지난 이야기 깊은 인공신경망의 문제점 중 하나인 vanishing gradient…\r\n이를 해결하기 위한 방안인 ReLU에 대해 살펴보았다.\r\n이번글에서는 또 다른 방안인 Batch Normalization에 대해 살펴보겠다. 1. Batch Normalization 자 이전시간에 배운 ReLU를 이용하여 신경망을 구성한다고 해보자.\r\nBatch Normalization의 batch는 mini-batch 알고리즘의 batch를 의미한다.\r\nbatch normalization은 어떤 layer에 적용할지를 선택한다. (이는 하이퍼 파라미터이다 ㅇㅇ)\r\n예시 그림에서는 첫번째 보이는 hidden layer에 batch normalization을 적용하려 한다.\r\nimage 예시니까 간단한 구조로 그린것임 자 이때 ReLU에 집중해보자.\r\n만약 ReLU에 들어온 데이터가 모두 양수였다면, 모두 linear하게 나갈것이다.\r\n그렇게 되면 이 액티베이션이 ReLU인지 linear ac…","frontmatter":{"date":"February 16, 2023","title":"16. 깊은 인공신경망의 문제 2편","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/17/"}},"next":{"id":"75ef1949-7e48-5147-b4bf-f5b31b62d400","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/15/\">이전글</a>에서는 Universal Approximation Theorem을 공부했다.<br>\r\n이번글에서는 깊은 인공신경망의 문제점과 그 해결방안에 대해 다뤄보고자 한다.</p>\n<br>\r\n<br>\n<h2 id=\"1-깊은-인공신경망의-문제점\" style=\"position:relative;\"><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\" aria-label=\"1 깊은 인공신경망의 문제점 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>1. 깊은 인공신경망의 문제점</h2>\n<p>지금까지 우리는 인공신경망을 이용해서 딥러닝에 적용하는 방법에 대해 배웠다.<br>\r\n지난글에서는 Universal Approximation Theorem을 통해서 어떠한 연속함수든 근사해낼 수 있음도 알게 되었다.<br>\r\n그렇다면 인공신경망은 만능일까?<br>\r\n깊게 만들면 어떠한 문제든 다 해결할 수 있을까?<br>\r\n아쉽게도 그렇지만은 않다. 아무래도 그냥 DNN을 만드는것보다 중요한것은 ”<strong>잘</strong>“만드는 것이다.\r\n<br><br>\r\n생각없이 만들면 어떠한 문제에 봉착하는 것일까?<br>\r\n우리는 크게 2가지의 문제에 봉착한다.</p>\n<ul>\n<li><strong>Vanishing Gradient</strong></li>\n<li><strong>Overfitting</strong></li>\n<li>(bonus) loss landscape 의 non-convexity</li>\n</ul>\n<blockquote>\n<p>마지막 bonus는 초심자 입장에서 생각할 문제는 아니다.<br>\r\nloss의 landscape를 가능한 convex하게 만들면 GD가 잘 동작할 것이니, 이러한 점도 고려할 수 있다면 고려하자.<br>\r\n하지만 우리는 초심자이니까 앞의 두개에 대한 이야기를 하도록 하겠다.</p>\n</blockquote>\n<br>\r\n<br>\n<h2 id=\"2-vanishing-gradient\" style=\"position:relative;\"><a href=\"#2-vanishing-gradient\" aria-label=\"2 vanishing gradient permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>2. Vanishing Gradient</h2>\n<p>vanishing gradient는 직역하면 ‘gradient가 사라지는것’ 정도로 해석할 수 있다.<br>\r\ngradient가 사라진다니? 직관적으로 설명하면, ”<strong>줄을 아무리 흔들어도 뒤까지 전달이 안된다</strong>“라고 비유할 수 있다. <br>\r\n혹시 backpropagation을 기억하는가? 깊은 곳의 weight는 chain rule이 연속적으로 적용되어 곱해지는 값들이 많다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219021253-cedaa2b6-0eac-46e0-8f44-d74db02adfb6.png\" alt=\"image\">\r\n<br></p>\n<blockquote>\n<p>기억나지 않는 분들을 위해서 가져왔다.</p>\n</blockquote>\n<p>자 그런데 생각해보자. 우리가 지금까지 배운 activation 중 미분이 가능한 activation은 linear activation과 sigmoid이다.<br>\r\n그런데 linear activation은 신경망의 깊이 자체에 영향을 주지 못한다는 사실을 배웠다. 즉 깊은 인공신경망을 위해서는 sigmoid가 이용되어야한다는 것인데..<br>\r\nsigmoid는 미분의 최대값이 1/4이다. 즉 저 많은 곱셈연산중 activation 미분값이 4분의 1 이하라는 것이다.<br>\r\n그렇다면 4분의 1이 3번만 곱해져도 1/64로 0에 급속히 가까워진다.<br>\r\n즉 weight의 업데이트가 거의 일어나지 않게 되는 것이다.<br></p>\n<blockquote>\n<p>정리 : 깊은 곳의 weight일수록, Loss의 미분값이 매우 작아서(0에 가까워서) 업데이트가 거의 일어나지 않는 현상이 일어난다.</p>\n</blockquote>\n<p>이를 Vanishing Gradient라고 한다.<br>\r\nvanishing gradient가 일어나면, 학습이 잘 이뤄지지않고 오히려 underfitting이 일어나게 된다.(오지게 안맞는다ㅇㅇ)<br></p>\n<br>\r\n<br>\n<h2 id=\"3-sigmoid가-범인-해결책은\" style=\"position:relative;\"><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\" aria-label=\"3 sigmoid가 범인 해결책은 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>3. Sigmoid가 범인! 해결책은?</h2>\n<p>vanishing gradient를 일으키는 범인은 누굴까? 역시 sigmoid의 작은 미분 값때문이 것이다.<br>\r\n따라서 이를 위해서 sigmoid를 포기하고 새로운 activation function을 찾아본다.<br>\r\n그리고 등장한 것이 바로 <strong>ReLU(Rectified Linear Unit)</strong> 이다.<br>\r\n그럼 ReLU는 어떻게 생겼을까? 매우 간단하게 생겼다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024070-aea3dc2c-2c4a-4f3c-8505-a83395fed72c.png\" alt=\"image\">\r\n<br>\r\n원점을 기준으로 음의 방향은 0, 양의 방향은 linear 한 모습을 갖고 있다.<br>\r\n<strong>그런데!</strong> 이런 모습을 띄고 있자니 문제가 있을 것 같다. 모든 input이 0보다 작으면 output이 0만 출력되는 극단적인 경우가 발생할 수 있다.<br>\r\n그래서 여러가지 변형된 버전의 ReLU가 등장하였는데 그중 대표적으로 Leaky ReLU와 Parametric ReLU가 있다.<br>\r\n<img src=\"https://user-images.githubusercontent.com/121401159/219024822-fe799045-23d1-4408-be9f-3534869d0a79.png\" alt=\"image\"><br></p>\n<blockquote>\n<p>leaky ReLU는 ‘다 0으로 죽이기 뭐하니까 살리자’라는 느낌<br>\r\nparametric ReLU는 ‘살리긴하는데 좀 근거 있게 살리자’라는 느낌</p>\n</blockquote>\n<p>parametric ReLU는 기울기를 학습을 통해서 결정한다. 즉 기울기 a도 미분에 참여한다.<br></p>\n<p><br><br></p>\n<h2 id=\"4-relu-그냥-다-살리면-안돼\" style=\"position:relative;\"><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\" aria-label=\"4 relu 그냥 다 살리면 안돼 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>4. ReLU! 그냥 다 살리면 안돼?</h2>\n<p>왜 애매하게 남겨두냐! 그냥 살리면 안되냐? 라고 생각하는 사람도 있을 수 있다. 그러나 그러한 경우 linear activation과 같아지므로 신경망이 깊어지는 효과를 누리지 못한다는 단점이 있다.<br>\r\n그래서 다시한번 정리하면, Linear activation은 이용될때 ‘마지막 출력 layer’에서만 쓰인다.(일반적으로 ㅇㅇ)<br></p>\n<br>\n<h2 id=\"5-relu의-자매품\" style=\"position:relative;\"><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\" aria-label=\"5 relu의 자매품 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>5. ReLU의 자매품</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219025975-74df55ff-b6eb-4d52-acbc-6ae49778a074.png\" alt=\"image\"><br>\r\nReLU 자매품이 다양하다. 관심이 있다면 하나씩 검색해서 살펴보자.</p>\n<p><br><br></p>\n<h2 id=\"6-마무리\" style=\"position:relative;\"><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\" aria-label=\"6 마무리 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>6. 마무리</h2>\n<p><img src=\"https://user-images.githubusercontent.com/121401159/219026480-1fca0e25-5b18-4b8c-98fd-ea8154aeee20.png\" alt=\"image\"><br></p>\n<p>이번글에서는 sigmoid의 한계로인해 새로운 activation ReLU에 대해서 살펴보았다.<br>\r\n<a href=\"https://tannybrown.github.io/ai/17/\">다음글</a>에서는 vanishing gradient를 해결할 다른 방안에 대해서 살펴보자.</p>\n<div class=\"table-of-contents\">\n<ul>\n<li><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\">0. 지난 이야기</a></li>\n<li><a href=\"#1-%EA%B9%8A%EC%9D%80-%EC%9D%B8%EA%B3%B5%EC%8B%A0%EA%B2%BD%EB%A7%9D%EC%9D%98-%EB%AC%B8%EC%A0%9C%EC%A0%90\">1. 깊은 인공신경망의 문제점</a></li>\n<li><a href=\"#2-vanishing-gradient\">2. Vanishing Gradient</a></li>\n<li><a href=\"#3-sigmoid%EA%B0%80-%EB%B2%94%EC%9D%B8-%ED%95%B4%EA%B2%B0%EC%B1%85%EC%9D%80\">3. Sigmoid가 범인! 해결책은?</a></li>\n<li><a href=\"#4-relu-%EA%B7%B8%EB%83%A5-%EB%8B%A4-%EC%82%B4%EB%A6%AC%EB%A9%B4-%EC%95%88%EB%8F%BC\">4. ReLU! 그냥 다 살리면 안돼?</a></li>\n<li><a href=\"#5-relu%EC%9D%98-%EC%9E%90%EB%A7%A4%ED%92%88\">5. ReLU의 자매품</a></li>\n<li><a href=\"#6-%EB%A7%88%EB%AC%B4%EB%A6%AC\">6. 마무리</a></li>\n</ul>\n</div>","frontmatter":{"date":"February 15, 2023","title":"15. 깊은 인공신경망의 문제 1편(feat. vanishing gradient)","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/16/"}},"prev":{"id":"168da2c2-0f3e-5f80-8b28-7efc66e0d9be","html":"<h2 id=\"0-지난-이야기\" style=\"position:relative;\"><a href=\"#0-%EC%A7%80%EB%82%9C-%EC%9D%B4%EC%95%BC%EA%B8%B0\" aria-label=\"0 지난 이야기 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>0. 지난 이야기</h2>\n<p><a href=\"https://tannybrown.github.io/ai/17/\">지난글</a>에서 vanishing gradient의 해결법에 대해 살펴보았다.<br>\r\n이번글에서는 나머지 문제인 overfitting을 해결하는 방법에 대해 이야기 해보자.<br></p>","frontmatter":{"date":"February 16, 2023","title":"17. 깊은 인공신경망의 문제 3편(feat. overfitting)","categories":"AI/ML/DL","author":"tanny","emoji":"🔮"},"fields":{"slug":"/ai/18/"}},"site":{"siteMetadata":{"siteUrl":"https://tannybrown.github.io","comments":{"utterances":{"repo":""}}}}},"pageContext":{"slug":"/ai/17/","nextSlug":"/ai/16/","prevSlug":"/ai/18/"}},
    "staticQueryHashes": ["1073350324","1956554647","2938748437"]}